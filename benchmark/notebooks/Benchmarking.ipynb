{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**BTB** provides a benchmarking framework that allows users and developers to evaluate the\n",
    "performance of the BTB Tuners or other tuning functions for Machine Learning Hyperparameter\n",
    "Tuning on hundreds of real world classification problem and classical mathematical optimization problems.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "In order to use this `benchmarking` module, you will have to fork the\n",
    "**BTB** repository\n",
    "and install it from source. You can visit our\n",
    "[Get Started](https://hdi-project.github.io/BTB/contributing.html#get-started)\n",
    "tutorial and follow until step 4, which explains how to clone and install the repository\n",
    "from it's source.\n",
    "\n",
    "### The Benchmarking process\n",
    "\n",
    "The Benchmarking BTB process has two main concepts.\n",
    "\n",
    "#### Challenges\n",
    "\n",
    "A Challenge of the BTB Benchmarking framework is a Python class which has a method that produces a\n",
    "score that can be optimized by tuning a set of hyperparameters.\n",
    "\n",
    "#### Tuning Functions\n",
    "\n",
    "In the context of the BTB Benchmarking, `Tuning Functions` are python functions that, given a scoring\n",
    "function and its tunable hyperparameters, try to search for the ideal hyperparameter values within\n",
    "a given number of iterations.\n",
    "\n",
    "If you want to add a tuner, you could follow the specific signature a tuning function has:\n",
    "\n",
    "```python3\n",
    "def tuning_function(\n",
    "    scoring_function: callable,\n",
    "    tunable_hyperparameters: dict,\n",
    "    iterations: int) -> score: float\n",
    "```\n",
    "\n",
    "## Creating a tuning function\n",
    "\n",
    "Now let's create a tuning function that takes as an input takes the following arguments:\n",
    "- `scoring_function`: A function that given the keyword args of the `tunable_hp` generates a score.\n",
    "- `tunable_hp`: A dictionary representation of `HyperParams`.\n",
    "- `iterations`: Amount of tuning iterations to perform.\n",
    "\n",
    "Our function will use a [BTB Tuner](https://github.com/HDI-Project/BTB/blob/master/tutorials/01_Tuning.ipynb) `GPEiTuner` that will iteratively will:\n",
    "\n",
    "1. Propose a new set of hyperparameters to be scored.\n",
    "2. Score them against the `scoring_function`.\n",
    "3. Update the `best_score` so far.\n",
    "\n",
    "And finally will return the `best_score` obtained for the given iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from btb.tuning import GPEiTuner, Tunable\n",
    "\n",
    "def tuning_function(scoring_function, tunable_hp, iterations):\n",
    "    tunable = Tunable.from_dict(tunable_hp)\n",
    "    tuner = GPEiTuner(tunable)\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        proposal = tuner.propose()\n",
    "        score = scoring_function(**proposal)\n",
    "        tuner.record(proposal, score)\n",
    "        \n",
    "        best_score = max(score, best_score)\n",
    "        \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Benchmarking\n",
    "\n",
    "The user API for the BTB Benchmarking is the `btb_benchmark.main.run_benchmark` function.\n",
    "\n",
    "The `run_benchmark` function accepts the following arguments:\n",
    "\n",
    "- `tuners`: list of tuners or tuning functions that will be benchmarked.\n",
    "- `challenge_types`: list of types of challenges that will be used for benchmark (optional).\n",
    "- `challenges`: list of names of challenges that will be benchmarked (optional).\n",
    "- `sample`: if specified, run the benchmark on a subset of the available challenges of the given size (optional).\n",
    "- `iterations`: the number of tuning iterations to perform per challenge and tuner.\n",
    "- `output_path`: If given, store the benchmark results in the given path as a CSV file.\n",
    "- `detailed_output`: If ``True`` a dataframe with the elapsed time, score and iterations will be returned.\n",
    "\n",
    "\n",
    "*Note*: as we want to provide a simple usage example, we will be demostrating the benchmarking\n",
    "functionality with a fixed amount of `samples` and low `iterations`.\n",
    "\n",
    "The easiest way to run the benchmarking process is to import `run_benchmark` and run it with the\n",
    "desired arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTB.GPEiTuner</th>\n",
       "      <th>BTB.GPTuner</th>\n",
       "      <th>BTB.UniformTuner</th>\n",
       "      <th>HyperOpt.rand</th>\n",
       "      <th>HyperOpt.tpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoostChallenge('no2_1.csv')</th>\n",
       "      <td>0.480845</td>\n",
       "      <td>0.532451</td>\n",
       "      <td>0.525448</td>\n",
       "      <td>0.382432</td>\n",
       "      <td>0.532245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               BTB.GPEiTuner  BTB.GPTuner  BTB.UniformTuner  \\\n",
       "XGBoostChallenge('no2_1.csv')       0.480845     0.532451          0.525448   \n",
       "\n",
       "                               HyperOpt.rand  HyperOpt.tpe  \n",
       "XGBoostChallenge('no2_1.csv')       0.382432      0.532245  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from btb_benchmark import run_benchmark\n",
    "\n",
    "scores = run_benchmark(sample=1, iterations=1)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuners\n",
    "\n",
    "If you want to run the benchmark on your own tuner implementation, or in a subset of the BTB\n",
    "tuners, you can pass them as a list to the tuners argument. This can be done by either directly\n",
    "passing the function or the name of the implemented tuners:\n",
    "\n",
    "- `BTB.UniformTuner`\n",
    "- `BTB.GPTuner`\n",
    "- `BTB.GPEiTuner`\n",
    "- `HyperOpt.rand`\n",
    "- `HyperOpt.tpe`\n",
    "\n",
    "For example, if we want to compare the performance of our tuning function and BTB.GPTuner, we can\n",
    "call the `run_benchmark` function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTB.GPTuner</th>\n",
       "      <th>tuning_function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoostChallenge('fri_c2_100_10_1.csv')</th>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.354839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         BTB.GPTuner  tuning_function\n",
       "XGBoostChallenge('fri_c2_100_10_1.csv')     0.354839         0.354839"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuners = [\n",
    "    tuning_function,\n",
    "    'BTB.GPTuner',\n",
    "]\n",
    "results = run_benchmark(tuners=tuners, sample=1, iterations=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenges\n",
    "\n",
    "If we want to run the benchmark on a subset of the challenges, we can pass their names to the\n",
    "challenges argument. If a given challenge is the name of a mathematical optimization problem\n",
    "function, the corresponding Mathematical Optimization Challenge will be executed.\n",
    "\n",
    "If the given challenge is the name of a Machine Learning Classification problem, all the\n",
    "implemented classifiers will be benchmarked on that dataset.\n",
    "\n",
    "For example, if we want to run only on the `stock_1` dataset, we can call the `run_benchmark` function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTB.GPEiTuner</th>\n",
       "      <th>BTB.GPTuner</th>\n",
       "      <th>BTB.UniformTuner</th>\n",
       "      <th>HyperOpt.rand</th>\n",
       "      <th>HyperOpt.tpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoostChallenge('stock_1')</th>\n",
       "      <td>0.792692</td>\n",
       "      <td>0.787453</td>\n",
       "      <td>0.79498</td>\n",
       "      <td>0.700596</td>\n",
       "      <td>0.632028</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             BTB.GPEiTuner  BTB.GPTuner  BTB.UniformTuner  \\\n",
       "XGBoostChallenge('stock_1')       0.792692     0.787453           0.79498   \n",
       "\n",
       "                             HyperOpt.rand  HyperOpt.tpe  \n",
       "XGBoostChallenge('stock_1')       0.700596      0.632028  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenges = ['stock_1']\n",
    "results = run_benchmark(challenges=challenges, iterations=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, if we only want to run on a family of challenges or a specific Machine Learning\n",
    "model, we can specify it passing the `types` argument.\n",
    "\n",
    "For example, if we want to run all the dataset on the XGBoost model, we can call the run benchmark\n",
    "function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTB.GPEiTuner</th>\n",
       "      <th>BTB.GPTuner</th>\n",
       "      <th>BTB.UniformTuner</th>\n",
       "      <th>HyperOpt.rand</th>\n",
       "      <th>HyperOpt.tpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBoostChallenge('witmer_census_1980_1.csv')</th>\n",
       "      <td>0.342017</td>\n",
       "      <td>0.342017</td>\n",
       "      <td>0.342017</td>\n",
       "      <td>0.342017</td>\n",
       "      <td>0.342017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              BTB.GPEiTuner  BTB.GPTuner  \\\n",
       "XGBoostChallenge('witmer_census_1980_1.csv')       0.342017     0.342017   \n",
       "\n",
       "                                              BTB.UniformTuner  HyperOpt.rand  \\\n",
       "XGBoostChallenge('witmer_census_1980_1.csv')          0.342017       0.342017   \n",
       "\n",
       "                                              HyperOpt.tpe  \n",
       "XGBoostChallenge('witmer_census_1980_1.csv')      0.342017  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = run_benchmark(challenge_types=['xgboost'], sample=1, iterations=5)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing the results\n",
    "\n",
    "If we want to store the obtained results directly in to a file, we can pass the path to where we\n",
    "would like to save our results, by adding the argument `output_path`.\n",
    "\n",
    "For example, if we want to store it as `path/to/my_results.csv` we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_benchmark(sample=1, iterations=1, output_path='my_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
