

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>btb.benchmark.challenges.wind module &mdash; BTB 0.3.4.dev0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/dai-logo-white.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="btb.benchmark.tuners package" href="btb.benchmark.tuners.html" />
    <link rel="prev" title="btb.benchmark.challenges.rosenbrock module" href="btb.benchmark.challenges.rosenbrock.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> BTB
          

          
            
            <img src="../_static/dai-logo-white-200.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../readme.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#install">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#install-using-pip">Install using Pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#install-from-source">Install from Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#install-for-development">Install for Development</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#quickstart">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#tuners">Tuners</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../readme.html#selectors">Selectors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#what-s-next">Whatâ€™s next?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#citing-btb">Citing BTB</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="btb.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="btb.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="btb.benchmark.html">btb.benchmark package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="btb.benchmark.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="btb.benchmark.html#module-btb.benchmark">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="btb.selection.html">btb.selection package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="btb.selection.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="btb.selection.html#module-btb.selection">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.html">btb.tuning package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="btb.tuning.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="btb.tuning.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="btb.tuning.html#module-btb.tuning">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="btb.html#module-btb">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#types-of-contributions">Types of Contributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#report-bugs">Report Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#fix-bugs">Fix Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#implement-features">Implement Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#write-documentation">Write Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#submit-feedback">Submit Feedback</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#unit-testing-guidelines">Unit Testing Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#tips">Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#release-workflow">Release Workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#release-candidates">Release Candidates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../history.html">History</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id1">0.3.3 - 2019-12-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#internal-improvements">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#resolved-issues">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id2">0.3.2 - 2019-12-10</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-features">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id3">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id4">0.3.1 - 2019-11-25</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id5">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id6">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id7">0.3.0 - 2019-11-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-project-structure">New project structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-api">New API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id8">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id9">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id10">0.2.5</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#bug-fixes">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id11">0.2.4</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id12">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id13">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id14">0.2.3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id15">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id16">0.2.2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id17">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id18">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id19">0.2.1</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id20">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id21">0.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id22">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id23">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id24">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id25">0.1.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id26">0.1.1</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">BTB</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="btb.html">btb package</a> &raquo;</li>
        
          <li><a href="btb.benchmark.html">btb.benchmark package</a> &raquo;</li>
        
          <li><a href="btb.benchmark.challenges.html">btb.benchmark.challenges package</a> &raquo;</li>
        
      <li>btb.benchmark.challenges.wind module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/HDI-Project/BTB/blob/master/docs/api/btb.benchmark.challenges.wind.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-btb.benchmark.challenges.wind">
<span id="btb-benchmark-challenges-wind-module"></span><h1>btb.benchmark.challenges.wind module<a class="headerlink" href="#module-btb.benchmark.challenges.wind" title="Permalink to this headline">Â¶</a></h1>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC" title="btb.benchmark.challenges.wind.WindABC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WindABC</span></code></a>([model,Â dataset,Â target_column,Â â€¦])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC" title="btb.benchmark.challenges.wind.WindRFC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WindRFC</span></code></a>([model,Â dataset,Â target_column,Â â€¦])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC" title="btb.benchmark.challenges.wind.WindSGDC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WindSGDC</span></code></a>([model,Â dataset,Â target_column,Â â€¦])</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="btb.benchmark.challenges.wind.WindABC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.wind.</code><code class="sig-name descname">WindABC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/wind.html#WindABC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC" title="btb.benchmark.challenges.wind.WindRFC"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.wind.WindRFC</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.DATASET" title="btb.benchmark.challenges.wind.WindABC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=â€™â€™) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.ENCODE" title="btb.benchmark.challenges.wind.WindABC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.MAKE_BINARY" title="btb.benchmark.challenges.wind.WindABC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.MODEL_DEFAULTS" title="btb.benchmark.challenges.wind.WindABC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.STRATIFIED" title="btb.benchmark.challenges.wind.WindABC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.TARGET_COLUMN" title="btb.benchmark.challenges.wind.WindABC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=â€™â€™) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.wind.WindABC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.METRIC" title="btb.benchmark.challenges.wind.WindABC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[,Â labels,Â pos_label,Â average,Â â€¦])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.evaluate" title="btb.benchmark.challenges.wind.WindABC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.get_tunable_hyperparameters" title="btb.benchmark.challenges.wind.WindABC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.load_data" title="btb.benchmark.challenges.wind.WindABC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindABC.MODEL" title="btb.benchmark.challenges.wind.WindABC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>An AdaBoost classifier.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindABC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'wind.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.DATASET" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindABC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.ENCODE" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindABC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.MAKE_BINARY" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindABC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.METRIC" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) â€“ Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) â€“ Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) â€“ <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) â€“ The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) â€“ <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters â€˜macroâ€™ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) â€“ Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> â€“ F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindABC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.MODEL" title="Permalink to this definition">Â¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.weight_boosting.AdaBoostClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindABC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.MODEL_DEFAULTS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindABC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.STRATIFIED" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindABC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'class'</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.TARGET_COLUMN" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindABC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'algorithm': {'default': 'SAMME.R', 'type': 'str', 'values': ['SAMME', 'SAMME.R']}, 'learning_rate': {'default': 1.0, 'range': [1.0, 10.0], 'type': 'float'}, 'n_estimators': {'default': 50, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindABC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.evaluate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) â€“ A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindABC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.get_tunable_hyperparameters" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindABC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindABC.load_data" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.wind.WindRFC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.wind.</code><code class="sig-name descname">WindRFC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/wind.html#WindRFC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.challenge.html#btb.benchmark.challenges.challenge.MLChallenge" title="btb.benchmark.challenges.challenge.MLChallenge"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.challenge.MLChallenge</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.DATASET" title="btb.benchmark.challenges.wind.WindRFC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=â€™â€™) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.ENCODE" title="btb.benchmark.challenges.wind.WindRFC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.MAKE_BINARY" title="btb.benchmark.challenges.wind.WindRFC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.MODEL_DEFAULTS" title="btb.benchmark.challenges.wind.WindRFC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.STRATIFIED" title="btb.benchmark.challenges.wind.WindRFC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.TARGET_COLUMN" title="btb.benchmark.challenges.wind.WindRFC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=â€™â€™) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.wind.WindRFC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.METRIC" title="btb.benchmark.challenges.wind.WindRFC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[,Â labels,Â pos_label,Â average,Â â€¦])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.evaluate" title="btb.benchmark.challenges.wind.WindRFC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.get_tunable_hyperparameters" title="btb.benchmark.challenges.wind.WindRFC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.load_data" title="btb.benchmark.challenges.wind.WindRFC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC.MODEL" title="btb.benchmark.challenges.wind.WindRFC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>A random forest classifier.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindRFC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'wind.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.DATASET" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindRFC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.ENCODE" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindRFC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.MAKE_BINARY" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindRFC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.METRIC" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) â€“ Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) â€“ Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) â€“ <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) â€“ The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) â€“ <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters â€˜macroâ€™ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) â€“ Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> â€“ F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindRFC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.MODEL" title="Permalink to this definition">Â¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.forest.RandomForestClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindRFC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.MODEL_DEFAULTS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindRFC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.STRATIFIED" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindRFC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'class'</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.TARGET_COLUMN" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindRFC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'criterion': {'default': 'gini', 'type': 'str', 'values': ['entropy', 'gini']}, 'max_features': {'default': None, 'type': 'str', 'values': [None, 'auto', 'log2', 'sqrt']}, 'min_impurity_decrease': {'default': 0.0, 'range': [0.0, 1000.0], 'type': 'float'}, 'min_samples_leaf': {'default': 1, 'range': [1, 100], 'type': 'int'}, 'min_samples_split': {'default': 2, 'range': [2, 100], 'type': 'int'}, 'min_weight_fraction_leaf': {'default': 0.0, 'range': [0.0, 0.5], 'type': 'float'}, 'n_estimators': {'default': 10, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindRFC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.evaluate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) â€“ A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindRFC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.get_tunable_hyperparameters" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindRFC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindRFC.load_data" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.wind.WindSGDC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.wind.</code><code class="sig-name descname">WindSGDC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/wind.html#WindSGDC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#btb.benchmark.challenges.wind.WindRFC" title="btb.benchmark.challenges.wind.WindRFC"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.wind.WindRFC</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.DATASET" title="btb.benchmark.challenges.wind.WindSGDC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=â€™â€™) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.ENCODE" title="btb.benchmark.challenges.wind.WindSGDC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.MAKE_BINARY" title="btb.benchmark.challenges.wind.WindSGDC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.MODEL_DEFAULTS" title="btb.benchmark.challenges.wind.WindSGDC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.STRATIFIED" title="btb.benchmark.challenges.wind.WindSGDC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.TARGET_COLUMN" title="btb.benchmark.challenges.wind.WindSGDC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=â€™â€™) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.wind.WindSGDC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.METRIC" title="btb.benchmark.challenges.wind.WindSGDC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[,Â labels,Â pos_label,Â average,Â â€¦])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.evaluate" title="btb.benchmark.challenges.wind.WindSGDC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.get_tunable_hyperparameters" title="btb.benchmark.challenges.wind.WindSGDC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.load_data" title="btb.benchmark.challenges.wind.WindSGDC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.wind.WindSGDC.MODEL" title="btb.benchmark.challenges.wind.WindSGDC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindSGDC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'wind.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.DATASET" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindSGDC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.ENCODE" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindSGDC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.MAKE_BINARY" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindSGDC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.METRIC" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) â€“ Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) â€“ Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) â€“ <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) â€“ The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) â€“ <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters â€˜macroâ€™ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) â€“ Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> â€“ F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindSGDC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.MODEL" title="Permalink to this definition">Â¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindSGDC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.MODEL_DEFAULTS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindSGDC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.STRATIFIED" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindSGDC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'class'</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.TARGET_COLUMN" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.wind.WindSGDC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'alpha': {'default': 0.0001, 'type': 'float', 'values': [0.0001, 1]}, 'loss': {'default': 'hinge', 'range': ['log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'], 'type': 'str'}, 'max_iter': {'default': 1000, 'type': 'int', 'values': [1, 5000]}, 'penalty': {'default': None, 'type': 'str', 'values': [None, 'l2', 'l1', 'elasticnet']}, 'shuffle': {'default': True, 'type': 'bool'}, 'tol': {'default': 0.001, 'type': 'float', 'values': [0.001, 1]}}</em><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindSGDC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.evaluate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) â€“ A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindSGDC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.get_tunable_hyperparameters" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.wind.WindSGDC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.wind.WindSGDC.load_data" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="btb.benchmark.tuners.html" class="btn btn-neutral float-right" title="btb.benchmark.tuners package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="btb.benchmark.challenges.rosenbrock.html" class="btn btn-neutral float-left" title="btb.benchmark.challenges.rosenbrock module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, MIT Data To AI Lab

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>