

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>btb.benchmark.challenges package &mdash; BTB 0.3.5.dev0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/dai-logo-white.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="btb.benchmark.challenges.bohachevsky module" href="btb.benchmark.challenges.bohachevsky.html" />
    <link rel="prev" title="btb.benchmark package" href="btb.benchmark.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> BTB
          

          
            
            <img src="../_static/dai-logo-white-200.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../readme.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#install">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#install-using-pip">Install using Pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#install-from-source">Install from Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#install-for-development">Install for Development</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../readme.html#quickstart">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#tuners">Tuners</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../readme.html#selectors">Selectors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#what-s-next">What’s next?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../readme.html#citing-btb">Citing BTB</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="btb.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="btb.html#subpackages">Subpackages</a><ul class="current">
<li class="toctree-l3 current"><a class="reference internal" href="btb.benchmark.html">btb.benchmark package</a><ul class="current">
<li class="toctree-l4 current"><a class="reference internal" href="btb.benchmark.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="btb.benchmark.html#module-btb.benchmark">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="btb.selection.html">btb.selection package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="btb.selection.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="btb.selection.html#module-btb.selection">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.html">btb.tuning package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="btb.tuning.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="btb.tuning.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="btb.tuning.html#module-btb.tuning">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="btb.html#submodules">Submodules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="btb.session.html">btb.session module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="btb.html#module-btb">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#types-of-contributions">Types of Contributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#report-bugs">Report Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#fix-bugs">Fix Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#implement-features">Implement Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#write-documentation">Write Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#submit-feedback">Submit Feedback</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#unit-testing-guidelines">Unit Testing Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#tips">Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#release-workflow">Release Workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#release-candidates">Release Candidates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../history.html">History</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id1">0.3.4 - 2019-12-24</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-features">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#internal-improvements">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#resolved-issues">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id2">0.3.3 - 2019-12-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id3">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id4">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id5">0.3.2 - 2019-12-10</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id6">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id7">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id8">0.3.1 - 2019-11-25</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id9">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id10">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id11">0.3.0 - 2019-11-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-project-structure">New project structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-api">New API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id12">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id13">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id14">0.2.5</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#bug-fixes">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id15">0.2.4</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id16">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id17">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id18">0.2.3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id19">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id20">0.2.2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id21">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id22">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id23">0.2.1</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id24">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id25">0.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id26">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id27">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id28">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id29">0.1.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id30">0.1.1</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">BTB</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="btb.html">btb package</a> &raquo;</li>
        
          <li><a href="btb.benchmark.html">btb.benchmark package</a> &raquo;</li>
        
      <li>btb.benchmark.challenges package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/HDI-Project/BTB/blob/master/docs/api/btb.benchmark.challenges.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="btb-benchmark-challenges-package">
<h1>btb.benchmark.challenges package<a class="headerlink" href="#btb-benchmark-challenges-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="btb.benchmark.challenges.bohachevsky.html">btb.benchmark.challenges.bohachevsky module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btb.benchmark.challenges.boston.html">btb.benchmark.challenges.boston module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btb.benchmark.challenges.branin.html">btb.benchmark.challenges.branin module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btb.benchmark.challenges.census.html">btb.benchmark.challenges.census module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btb.benchmark.challenges.challenge.html">btb.benchmark.challenges.challenge module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btb.benchmark.challenges.rosenbrock.html">btb.benchmark.challenges.rosenbrock module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btb.benchmark.challenges.wind.html">btb.benchmark.challenges.wind module</a></li>
</ul>
</div>
</div>
<div class="section" id="module-btb.benchmark.challenges">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-btb.benchmark.challenges" title="Permalink to this headline">¶</a></h2>
<p>Top level where all the challenges are imported.</p>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Bohachevsky" title="btb.benchmark.challenges.Bohachevsky"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Bohachevsky</span></code></a>([min_x, max_x, min_y, max_y])</p></td>
<td><p>Bohachevsky challenge.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR" title="btb.benchmark.challenges.BostonABR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BostonABR</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR" title="btb.benchmark.challenges.BostonBR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BostonBR</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR" title="btb.benchmark.challenges.BostonRFR"><code class="xref py py-obj docutils literal notranslate"><span class="pre">BostonRFR</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p>This dataset contains information collected by the U.S Census Service concerning housing in the area of Boston.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Branin" title="btb.benchmark.challenges.Branin"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Branin</span></code></a>([a, b, c, r, s, t, min_x, max_x, …])</p></td>
<td><p>Branin challenge.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC" title="btb.benchmark.challenges.CensusABC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CensusABC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC" title="btb.benchmark.challenges.CensusRFC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CensusRFC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p>The Census challenge is based on the <cite>Census Income Dataset</cite>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC" title="btb.benchmark.challenges.CensusSGDC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CensusSGDC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Rosenbrock" title="btb.benchmark.challenges.Rosenbrock"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Rosenbrock</span></code></a>([a, b, min_x, max_x, min_y, max_y])</p></td>
<td><p>Rosenbrock Challenge.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC" title="btb.benchmark.challenges.WindABC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WindABC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC" title="btb.benchmark.challenges.WindRFC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WindRFC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC" title="btb.benchmark.challenges.WindSGDC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">WindSGDC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="btb.benchmark.challenges.Bohachevsky">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">Bohachevsky</code><span class="sig-paren">(</span><em class="sig-param">min_x=-100</em>, <em class="sig-param">max_x=100</em>, <em class="sig-param">min_y=-100</em>, <em class="sig-param">max_y=100</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/bohachevsky.html#Bohachevsky"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Bohachevsky" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.challenge.html#btb.benchmark.challenges.challenge.Challenge" title="btb.benchmark.challenges.challenge.Challenge"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.challenge.Challenge</span></code></a></p>
<p>Bohachevsky challenge.</p>
<p>The Bohachevsky functions are bowl shape functions. This function is usually evaluated on the
input domain <span class="math notranslate nohighlight">\(x \epsilon [-100, 100], y \epsilon [-100, 100]\)</span>.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://www.sfu.ca/~ssurjano/boha.html">https://www.sfu.ca/~ssurjano/boha.html</a></p>
</dd>
<dt>The function is defined by:</dt><dd><p><span class="math notranslate nohighlight">\(f(x, y) = x^2 + 2y^2 -0.3cos(3\pi x)-0.4cos(4\pi y)+0.7\)</span></p>
</dd>
<dt>It has one local minimum at:</dt><dd><p><span class="math notranslate nohighlight">\((x, y) = (0, 0)\)</span> where <span class="math notranslate nohighlight">\(f(x, y) = 0\)</span>.</p>
</dd>
</dl>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Bohachevsky.evaluate" title="btb.benchmark.challenges.Bohachevsky.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(x, y)</p></td>
<td><p>Perform evaluation for the given <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Bohachevsky.get_tunable_hyperparameters" title="btb.benchmark.challenges.Bohachevsky.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>min_x</strong> (<em>int</em>) – Minimum number that the hyperparameter can propouse for <code class="docutils literal notranslate"><span class="pre">x</span></code>. Defaults to -100.</p></li>
<li><p><strong>max_x</strong> (<em>int</em>) – Maximum number that the hyperparameter can propouse for <code class="docutils literal notranslate"><span class="pre">x</span></code>. Defaults to 100.</p></li>
<li><p><strong>min_y</strong> (<em>int</em>) – Minimum number that the hyperparameter can propouse for <code class="docutils literal notranslate"><span class="pre">y</span></code>. Defaults to -100.</p></li>
<li><p><strong>max_y</strong> (<em>int</em>) – Maximum number that the hyperparameter can propouse for <code class="docutils literal notranslate"><span class="pre">y</span></code>. Defaults to 100.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="btb.benchmark.challenges.Bohachevsky.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/bohachevsky.html#Bohachevsky.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Bohachevsky.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform evaluation for the given <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p>
<p>This method will score a result with a given configuration, then return the score obtained
for those <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.Bohachevsky.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/bohachevsky.html#Bohachevsky.get_tunable_hyperparameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Bohachevsky.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.BostonABR">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">BostonABR</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/boston.html#BostonABR"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.BostonABR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.boston.html#btb.benchmark.challenges.boston.BostonRFR" title="btb.benchmark.challenges.boston.BostonRFR"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.boston.BostonRFR</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.DATASET" title="btb.benchmark.challenges.BostonABR.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.ENCODE" title="btb.benchmark.challenges.BostonABR.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.MAKE_BINARY" title="btb.benchmark.challenges.BostonABR.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.MODEL_DEFAULTS" title="btb.benchmark.challenges.BostonABR.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.STRATIFIED" title="btb.benchmark.challenges.BostonABR.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.TARGET_COLUMN" title="btb.benchmark.challenges.BostonABR.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.BostonABR.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.METRIC" title="btb.benchmark.challenges.BostonABR.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, sample_weight, multioutput])</p></td>
<td><p>R^2 (coefficient of determination) regression score function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.evaluate" title="btb.benchmark.challenges.BostonABR.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.get_tunable_hyperparameters" title="btb.benchmark.challenges.BostonABR.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.load_data" title="btb.benchmark.challenges.BostonABR.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonABR.MODEL" title="btb.benchmark.challenges.BostonABR.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>An AdaBoost regressor.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonABR.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'boston.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonABR.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonABR.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = False</em><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonABR.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">sample_weight=None</em>, <em class="sig-param">multioutput='uniform_average'</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>R^2 (coefficient of determination) regression score function.</p>
<p>Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>)</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>string in</em><em> [</em><em>'raw_values'</em><em>, </em><em>'uniform_average'</em><em>, </em><em>'variance_weighted'</em><em>] or </em><em>None</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_outputs</em><em>)</em>) – <p>Defines aggregating of multiple output scores.
Array-like value defines weights used to average scores.
Default is “uniform_average”.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of scores in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Scores of all outputs are averaged with uniform weight.</p>
</dd>
<dt>’variance_weighted’ :</dt><dd><p>Scores of all outputs are averaged, weighted by the variances
of each individual output.</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span>Default value of multioutput is ‘uniform_average’.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>z</strong> – The R^2 score or ndarray of scores if ‘multioutput’ is
‘raw_values’.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This is not a symmetric function.</p>
<p>Unlike most other scores, R^2 score may be negative (it need not actually
be the square of a quantity R).</p>
<p>This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Wikipedia entry on the Coefficient of determination</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.948...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
<span class="gp">... </span>         <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span> 
<span class="go">0.938...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">-3.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonABR.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.weight_boosting.AdaBoostRegressor</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonABR.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonABR.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = False</em><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonABR.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'medv'</em><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonABR.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'learning_rate': {'default': 1.0, 'range': [1.0, 10.0], 'type': 'float'}, 'loss': {'default': 'linear', 'type': 'str', 'values': ['linear', 'square', 'exponential']}, 'n_estimators': {'default': 50, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonABR.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonABR.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonABR.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonABR.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.BostonBR">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">BostonBR</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/boston.html#BostonBR"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.BostonBR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.boston.html#btb.benchmark.challenges.boston.BostonRFR" title="btb.benchmark.challenges.boston.BostonRFR"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.boston.BostonRFR</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.DATASET" title="btb.benchmark.challenges.BostonBR.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.ENCODE" title="btb.benchmark.challenges.BostonBR.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.MAKE_BINARY" title="btb.benchmark.challenges.BostonBR.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.MODEL_DEFAULTS" title="btb.benchmark.challenges.BostonBR.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.STRATIFIED" title="btb.benchmark.challenges.BostonBR.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.TARGET_COLUMN" title="btb.benchmark.challenges.BostonBR.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.BostonBR.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.METRIC" title="btb.benchmark.challenges.BostonBR.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, sample_weight, multioutput])</p></td>
<td><p>R^2 (coefficient of determination) regression score function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.evaluate" title="btb.benchmark.challenges.BostonBR.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.get_tunable_hyperparameters" title="btb.benchmark.challenges.BostonBR.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.load_data" title="btb.benchmark.challenges.BostonBR.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonBR.MODEL" title="btb.benchmark.challenges.BostonBR.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>A Bagging regressor.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonBR.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'boston.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonBR.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonBR.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = False</em><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonBR.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">sample_weight=None</em>, <em class="sig-param">multioutput='uniform_average'</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>R^2 (coefficient of determination) regression score function.</p>
<p>Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>)</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>string in</em><em> [</em><em>'raw_values'</em><em>, </em><em>'uniform_average'</em><em>, </em><em>'variance_weighted'</em><em>] or </em><em>None</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_outputs</em><em>)</em>) – <p>Defines aggregating of multiple output scores.
Array-like value defines weights used to average scores.
Default is “uniform_average”.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of scores in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Scores of all outputs are averaged with uniform weight.</p>
</dd>
<dt>’variance_weighted’ :</dt><dd><p>Scores of all outputs are averaged, weighted by the variances
of each individual output.</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span>Default value of multioutput is ‘uniform_average’.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>z</strong> – The R^2 score or ndarray of scores if ‘multioutput’ is
‘raw_values’.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This is not a symmetric function.</p>
<p>Unlike most other scores, R^2 score may be negative (it need not actually
be the square of a quantity R).</p>
<p>This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Wikipedia entry on the Coefficient of determination</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.948...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
<span class="gp">... </span>         <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span> 
<span class="go">0.938...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">-3.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonBR.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.bagging.BaggingRegressor</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonBR.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'bootstrap': True, 'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonBR.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = False</em><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonBR.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'medv'</em><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonBR.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'max_samples': {'default': 1, 'range': [1, 100], 'type': 'int'}, 'n_estimators': {'default': 10, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonBR.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonBR.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonBR.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonBR.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.BostonRFR">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">BostonRFR</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/boston.html#BostonRFR"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.challenge.html#btb.benchmark.challenges.challenge.MLChallenge" title="btb.benchmark.challenges.challenge.MLChallenge"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.challenge.MLChallenge</span></code></a></p>
<p>This dataset contains information collected by the U.S Census Service
concerning housing in the area of Boston. It was obtained from the
StatLib archive <a class="reference external" href="http://lib.stat.cmu.edu/datasets/boston">http://lib.stat.cmu.edu/datasets/boston</a></p>
<p>The prediction task is to determine the price of a house based on the
location and other features that the dataset provides.</p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.DATASET" title="btb.benchmark.challenges.BostonRFR.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.ENCODE" title="btb.benchmark.challenges.BostonRFR.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.MAKE_BINARY" title="btb.benchmark.challenges.BostonRFR.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.MODEL_DEFAULTS" title="btb.benchmark.challenges.BostonRFR.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.STRATIFIED" title="btb.benchmark.challenges.BostonRFR.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.TARGET_COLUMN" title="btb.benchmark.challenges.BostonRFR.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.BostonRFR.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.METRIC" title="btb.benchmark.challenges.BostonRFR.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, sample_weight, multioutput])</p></td>
<td><p>R^2 (coefficient of determination) regression score function.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.evaluate" title="btb.benchmark.challenges.BostonRFR.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.get_tunable_hyperparameters" title="btb.benchmark.challenges.BostonRFR.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.load_data" title="btb.benchmark.challenges.BostonRFR.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.BostonRFR.MODEL" title="btb.benchmark.challenges.BostonRFR.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>A random forest regressor.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonRFR.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'boston.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonRFR.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonRFR.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = False</em><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonRFR.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">sample_weight=None</em>, <em class="sig-param">multioutput='uniform_average'</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>R^2 (coefficient of determination) regression score function.</p>
<p>Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>) or </em><em>(</em><em>n_samples</em><em>, </em><em>n_outputs</em><em>)</em>) – Estimated target values.</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> (</em><em>n_samples</em><em>)</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
<li><p><strong>multioutput</strong> (<em>string in</em><em> [</em><em>'raw_values'</em><em>, </em><em>'uniform_average'</em><em>, </em><em>'variance_weighted'</em><em>] or </em><em>None</em><em> or </em><em>array-like of shape</em><em> (</em><em>n_outputs</em><em>)</em>) – <p>Defines aggregating of multiple output scores.
Array-like value defines weights used to average scores.
Default is “uniform_average”.</p>
<dl class="simple">
<dt>’raw_values’ :</dt><dd><p>Returns a full set of scores in case of multioutput input.</p>
</dd>
<dt>’uniform_average’ :</dt><dd><p>Scores of all outputs are averaged with uniform weight.</p>
</dd>
<dt>’variance_weighted’ :</dt><dd><p>Scores of all outputs are averaged, weighted by the variances
of each individual output.</p>
</dd>
</dl>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.19: </span>Default value of multioutput is ‘uniform_average’.</p>
</div>
</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>z</strong> – The R^2 score or ndarray of scores if ‘multioutput’ is
‘raw_values’.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or ndarray of floats</p>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>This is not a symmetric function.</p>
<p>Unlike most other scores, R^2 score may be negative (it need not actually
be the square of a quantity R).</p>
<p>This metric is not well-defined for single samples and will return a NaN
value if n_samples is less than two.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Coefficient_of_determination">Wikipedia entry on the Coefficient of determination</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">7</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>  
<span class="go">0.948...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span>
<span class="gp">... </span>         <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span> 
<span class="go">0.938...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">1.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">0.0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="go">-3.0</span>
</pre></div>
</div>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonRFR.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.forest.RandomForestRegressor</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonRFR.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonRFR.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = False</em><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonRFR.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'medv'</em><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.BostonRFR.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'criterion': {'default': 'mse', 'type': 'str', 'values': ['mse', 'mae']}, 'max_features': {'default': 'auto', 'range': [None, 'auto', 'log2', 'sqrt'], 'type': 'str'}, 'min_impurity_decrease': {'default': 0.0, 'range': [0.0, 10.0], 'type': 'float'}, 'min_samples_leaf': {'default': 1, 'range': [1, 1000], 'type': 'int'}, 'min_samples_split': {'default': 2, 'range': [2, 1000], 'type': 'int'}, 'min_weight_fraction_leaf': {'default': 0.0, 'range': [0.0, 0.5], 'type': 'float'}, 'n_estimators': {'default': 10, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonRFR.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonRFR.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.BostonRFR.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.BostonRFR.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.Branin">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">Branin</code><span class="sig-paren">(</span><em class="sig-param">a=1</em>, <em class="sig-param">b=0.12918450914398066</em>, <em class="sig-param">c=1.5915494309189535</em>, <em class="sig-param">r=6</em>, <em class="sig-param">s=10</em>, <em class="sig-param">t=0.039788735772973836</em>, <em class="sig-param">min_x=-5.0</em>, <em class="sig-param">max_x=10.0</em>, <em class="sig-param">min_y=0.0</em>, <em class="sig-param">max_y=15.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/branin.html#Branin"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Branin" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.challenge.html#btb.benchmark.challenges.challenge.Challenge" title="btb.benchmark.challenges.challenge.Challenge"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.challenge.Challenge</span></code></a></p>
<p>Branin challenge.</p>
<p>The Branin, or Branin-Hoo, function is commonly used as a test function for metamodeling
in computer experiments, especially in the context of optimization. This function takes as
input <code class="docutils literal notranslate"><span class="pre">x</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> and has six constants that are named as <code class="docutils literal notranslate"><span class="pre">a,</span> <span class="pre">b,</span> <span class="pre">c,</span> <span class="pre">r,</span> <span class="pre">s,</span> <span class="pre">t</span></code>.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://uqworld.org/t/branin-function/53https://uqworld.org/t/branin-function/53">https://uqworld.org/t/branin-function/53https://uqworld.org/t/branin-function/53</a></p>
</dd>
<dt>The function is defined by:</dt><dd><p><span class="math notranslate nohighlight">\(f(x, y)=\left(y-\frac{5.1 x^2_1}{4 \pi^2}+\frac{5 x}{\pi}-6\right)^2+10\left(1-
\frac{1}{8 \pi}\right) \cos(x)+10\)</span></p>
</dd>
<dt>It has a global minimum, with the default <cite>a, b, c, r, s, t</cite> at the following three points:</dt><dd><p><span class="math notranslate nohighlight">\(f(x, y) = 0.397887\)</span> at <span class="math notranslate nohighlight">\(x, y = (-\pi, 12.275), (\pi, 2.275)\)</span> and
<span class="math notranslate nohighlight">\((9.42478, 2.475)\)</span></p>
</dd>
</dl>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Branin.evaluate" title="btb.benchmark.challenges.Branin.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(x, y)</p></td>
<td><p>Perform evaluation for the given <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Branin.get_tunable_hyperparameters" title="btb.benchmark.challenges.Branin.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<em>int</em>) – Constant value for <code class="docutils literal notranslate"><span class="pre">a</span></code>. Defaults to 1.</p></li>
<li><p><strong>b</strong> (<em>float</em><em> or </em><em>int</em>) – Constant value for <code class="docutils literal notranslate"><span class="pre">b</span></code>. Defaults to <span class="math notranslate nohighlight">\(5.1 / (4*\pi^2)\)</span>.</p></li>
<li><p><strong>c</strong> (<em>float</em><em> or </em><em>int</em>) – Constant value for <code class="docutils literal notranslate"><span class="pre">c</span></code>. Defaults to <span class="math notranslate nohighlight">\(5 / \pi\)</span>.</p></li>
<li><p><strong>r</strong> (<em>int</em>) – Constant value for <code class="docutils literal notranslate"><span class="pre">r</span></code>. Defaults to 6</p></li>
<li><p><strong>s</strong> (<em>int</em>) – Constant value for <code class="docutils literal notranslate"><span class="pre">s</span></code>. Defaults to 10</p></li>
<li><p><strong>t</strong> (<em>float</em><em> or </em><em>int</em>) – Constant value for <code class="docutils literal notranslate"><span class="pre">t</span></code>. Defaults to <span class="math notranslate nohighlight">\(1 / 8*\pi\)</span>.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="btb.benchmark.challenges.Branin.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/branin.html#Branin.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Branin.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform evaluation for the given <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p>
<p>This method will score a result with a given configuration, then return the score obtained
for those <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.Branin.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/branin.html#Branin.get_tunable_hyperparameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Branin.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.CensusABC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">CensusABC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/census.html#CensusABC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.CensusABC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.census.html#btb.benchmark.challenges.census.CensusRFC" title="btb.benchmark.challenges.census.CensusRFC"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.census.CensusRFC</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.DATASET" title="btb.benchmark.challenges.CensusABC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.ENCODE" title="btb.benchmark.challenges.CensusABC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.MAKE_BINARY" title="btb.benchmark.challenges.CensusABC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.MODEL_DEFAULTS" title="btb.benchmark.challenges.CensusABC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.STRATIFIED" title="btb.benchmark.challenges.CensusABC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.TARGET_COLUMN" title="btb.benchmark.challenges.CensusABC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.CensusABC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.METRIC" title="btb.benchmark.challenges.CensusABC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.evaluate" title="btb.benchmark.challenges.CensusABC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.get_tunable_hyperparameters" title="btb.benchmark.challenges.CensusABC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.load_data" title="btb.benchmark.challenges.CensusABC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusABC.MODEL" title="btb.benchmark.challenges.CensusABC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>An AdaBoost classifier.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusABC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'census.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusABC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusABC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusABC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id6"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusABC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.weight_boosting.AdaBoostClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusABC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusABC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusABC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'income'</em><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusABC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'algorithm': {'default': 'SAMME.R', 'type': 'str', 'values': ['SAMME', 'SAMME.R']}, 'learning_rate': {'default': 1.0, 'range': [1.0, 10.0], 'type': 'float'}, 'n_estimators': {'default': 50, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusABC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusABC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusABC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusABC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.CensusRFC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">CensusRFC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/census.html#CensusRFC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.challenge.html#btb.benchmark.challenges.challenge.MLChallenge" title="btb.benchmark.challenges.challenge.MLChallenge"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.challenge.MLChallenge</span></code></a></p>
<p>The Census challenge is based on the <cite>Census Income Dataset</cite>. The extraction
was done by Barry Becker from the 1994 Census Database. The prediction task
is to determine whether a person makes over 50.000 USD a year.</p>
<p>This dataset has been obtained from <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/census+income">https://archive.ics.uci.edu/ml/datasets/census+income</a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.DATASET" title="btb.benchmark.challenges.CensusRFC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.ENCODE" title="btb.benchmark.challenges.CensusRFC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.MAKE_BINARY" title="btb.benchmark.challenges.CensusRFC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.MODEL_DEFAULTS" title="btb.benchmark.challenges.CensusRFC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.STRATIFIED" title="btb.benchmark.challenges.CensusRFC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.TARGET_COLUMN" title="btb.benchmark.challenges.CensusRFC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.CensusRFC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.METRIC" title="btb.benchmark.challenges.CensusRFC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.evaluate" title="btb.benchmark.challenges.CensusRFC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.get_tunable_hyperparameters" title="btb.benchmark.challenges.CensusRFC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.load_data" title="btb.benchmark.challenges.CensusRFC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusRFC.MODEL" title="btb.benchmark.challenges.CensusRFC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>A random forest classifier.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusRFC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'census.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusRFC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusRFC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusRFC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusRFC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.forest.RandomForestClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusRFC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusRFC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusRFC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'income'</em><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusRFC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'criterion': {'default': 'gini', 'type': 'str', 'values': ['entropy', 'gini']}, 'max_features': {'default': None, 'type': 'str', 'values': [None, 'auto', 'log2', 'sqrt']}, 'min_impurity_decrease': {'default': 0.0, 'range': [0.0, 1000.0], 'type': 'float'}, 'min_samples_leaf': {'default': 1, 'range': [1, 100], 'type': 'int'}, 'min_samples_split': {'default': 2, 'range': [2, 100], 'type': 'int'}, 'min_weight_fraction_leaf': {'default': 0.0, 'range': [0.0, 0.5], 'type': 'float'}, 'n_estimators': {'default': 10, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusRFC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusRFC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusRFC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusRFC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.CensusSGDC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">CensusSGDC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/census.html#CensusSGDC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.census.html#btb.benchmark.challenges.census.CensusRFC" title="btb.benchmark.challenges.census.CensusRFC"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.census.CensusRFC</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.DATASET" title="btb.benchmark.challenges.CensusSGDC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.ENCODE" title="btb.benchmark.challenges.CensusSGDC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.MAKE_BINARY" title="btb.benchmark.challenges.CensusSGDC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.MODEL_DEFAULTS" title="btb.benchmark.challenges.CensusSGDC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.STRATIFIED" title="btb.benchmark.challenges.CensusSGDC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.TARGET_COLUMN" title="btb.benchmark.challenges.CensusSGDC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.CensusSGDC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.METRIC" title="btb.benchmark.challenges.CensusSGDC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.evaluate" title="btb.benchmark.challenges.CensusSGDC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.get_tunable_hyperparameters" title="btb.benchmark.challenges.CensusSGDC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.load_data" title="btb.benchmark.challenges.CensusSGDC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.CensusSGDC.MODEL" title="btb.benchmark.challenges.CensusSGDC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusSGDC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'census.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusSGDC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusSGDC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusSGDC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id9"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusSGDC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusSGDC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusSGDC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusSGDC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'income'</em><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.CensusSGDC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'alpha': {'default': 0.0001, 'type': 'float', 'values': [0.0001, 1]}, 'loss': {'default': 'hinge', 'range': ['log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'], 'type': 'str'}, 'max_iter': {'default': 1000, 'type': 'int', 'values': [1, 5000]}, 'penalty': {'default': None, 'type': 'str', 'values': [None, 'l2', 'l1', 'elasticnet']}, 'shuffle': {'default': True, 'type': 'bool'}, 'tol': {'default': 0.001, 'type': 'float', 'values': [0.001, 1]}}</em><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusSGDC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusSGDC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.CensusSGDC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.CensusSGDC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.Rosenbrock">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">Rosenbrock</code><span class="sig-paren">(</span><em class="sig-param">a=1</em>, <em class="sig-param">b=100</em>, <em class="sig-param">min_x=-50</em>, <em class="sig-param">max_x=50</em>, <em class="sig-param">min_y=-50</em>, <em class="sig-param">max_y=50</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/rosenbrock.html#Rosenbrock"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Rosenbrock" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.challenge.html#btb.benchmark.challenges.challenge.Challenge" title="btb.benchmark.challenges.challenge.Challenge"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.challenge.Challenge</span></code></a></p>
<p>Rosenbrock Challenge.</p>
<p>This challenge represents the Rosenbrock function, this is a non-convex function, introduced by
Howard H. Rosenbrock in 1960, which is used as a performance test problem for optimization
algorithms.[1] It is also known as Rosenbrock’s valley or Rosenbrock’s banana function.</p>
<p>The global minimum is inside a long, narrow, parabolic shaped flat valley. To find the valley
is trivial. To converge to the global minimum, however, is difficult.</p>
<dl class="simple">
<dt>Reference:</dt><dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Rosenbrock_function">https://en.wikipedia.org/wiki/Rosenbrock_function</a></p>
</dd>
<dt>The function is defined by:</dt><dd><p><span class="math notranslate nohighlight">\(f(x, y) = (a - x)^2 + b(y - x ^2)^2\)</span></p>
</dd>
<dt>It has a global minimum at:</dt><dd><p><span class="math notranslate nohighlight">\((x, y) = (a, a^2)\)</span> where <cite>f(x, y) = 0</cite>. Usually these parameters are set such that
<span class="math notranslate nohighlight">\(a = 1\)</span> and <span class="math notranslate nohighlight">\(b = 100\)</span>. Only in the trivial case where <span class="math notranslate nohighlight">\(a = 0\)</span> is the
function symmetric and the minimum at the origin.</p>
</dd>
</dl>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Rosenbrock.evaluate" title="btb.benchmark.challenges.Rosenbrock.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(x, y)</p></td>
<td><p>Perform evaluation for the given <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.Rosenbrock.get_tunable_hyperparameters" title="btb.benchmark.challenges.Rosenbrock.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
</tbody>
</table>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>a</strong> (<em>int</em>) – Number that <code class="docutils literal notranslate"><span class="pre">a</span></code> will take in the function. Defaults to 1.</p></li>
<li><p><strong>b</strong> (<em>int</em>) – Number that <code class="docutils literal notranslate"><span class="pre">b</span></code> will take in the function. Defaults to 100.</p></li>
<li><p><strong>min_x</strong> (<em>int</em>) – Minimum number that the hyperparameter can propouse for <code class="docutils literal notranslate"><span class="pre">x</span></code>. Defaults to -50.</p></li>
<li><p><strong>max_x</strong> (<em>int</em>) – Maximum number that the hyperparameter can propouse for <code class="docutils literal notranslate"><span class="pre">x</span></code>. Defaults to 50.</p></li>
<li><p><strong>min_y</strong> (<em>int</em>) – Minimum number that the hyperparameter can propouse for <code class="docutils literal notranslate"><span class="pre">y</span></code>. Defaults to -50.</p></li>
<li><p><strong>max_y</strong> (<em>int</em>) – Maximum number that the hyperparameter can propouse for <code class="docutils literal notranslate"><span class="pre">y</span></code>. Defaults to 50.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="btb.benchmark.challenges.Rosenbrock.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">y</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/rosenbrock.html#Rosenbrock.evaluate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Rosenbrock.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Perform evaluation for the given <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p>
<p>This method will score a result with a given configuration, then return the score obtained
for those <code class="docutils literal notranslate"><span class="pre">arguments</span></code>.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.Rosenbrock.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/rosenbrock.html#Rosenbrock.get_tunable_hyperparameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.Rosenbrock.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.WindABC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">WindABC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/wind.html#WindABC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.WindABC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.wind.html#btb.benchmark.challenges.wind.WindRFC" title="btb.benchmark.challenges.wind.WindRFC"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.wind.WindRFC</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.DATASET" title="btb.benchmark.challenges.WindABC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.ENCODE" title="btb.benchmark.challenges.WindABC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.MAKE_BINARY" title="btb.benchmark.challenges.WindABC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.MODEL_DEFAULTS" title="btb.benchmark.challenges.WindABC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.STRATIFIED" title="btb.benchmark.challenges.WindABC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.TARGET_COLUMN" title="btb.benchmark.challenges.WindABC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.WindABC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.METRIC" title="btb.benchmark.challenges.WindABC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.evaluate" title="btb.benchmark.challenges.WindABC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.get_tunable_hyperparameters" title="btb.benchmark.challenges.WindABC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.load_data" title="btb.benchmark.challenges.WindABC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindABC.MODEL" title="btb.benchmark.challenges.WindABC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>An AdaBoost classifier.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.WindABC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'wind.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.WindABC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindABC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindABC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindABC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindABC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindABC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindABC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id11"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindABC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.WindABC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.weight_boosting.AdaBoostClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindABC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.WindABC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindABC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindABC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindABC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'class'</em><a class="headerlink" href="#btb.benchmark.challenges.WindABC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindABC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'algorithm': {'default': 'SAMME.R', 'type': 'str', 'values': ['SAMME', 'SAMME.R']}, 'learning_rate': {'default': 1.0, 'range': [1.0, 10.0], 'type': 'float'}, 'n_estimators': {'default': 50, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.WindABC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindABC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindABC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindABC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindABC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindABC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindABC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.WindRFC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">WindRFC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/wind.html#WindRFC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.WindRFC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.challenge.html#btb.benchmark.challenges.challenge.MLChallenge" title="btb.benchmark.challenges.challenge.MLChallenge"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.challenge.MLChallenge</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.DATASET" title="btb.benchmark.challenges.WindRFC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.ENCODE" title="btb.benchmark.challenges.WindRFC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.MAKE_BINARY" title="btb.benchmark.challenges.WindRFC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.MODEL_DEFAULTS" title="btb.benchmark.challenges.WindRFC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.STRATIFIED" title="btb.benchmark.challenges.WindRFC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.TARGET_COLUMN" title="btb.benchmark.challenges.WindRFC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.WindRFC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.METRIC" title="btb.benchmark.challenges.WindRFC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.evaluate" title="btb.benchmark.challenges.WindRFC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.get_tunable_hyperparameters" title="btb.benchmark.challenges.WindRFC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.load_data" title="btb.benchmark.challenges.WindRFC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindRFC.MODEL" title="btb.benchmark.challenges.WindRFC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>A random forest classifier.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.WindRFC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'wind.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindRFC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindRFC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindRFC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id13"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindRFC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.forest.RandomForestClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindRFC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindRFC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindRFC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'class'</em><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindRFC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'criterion': {'default': 'gini', 'type': 'str', 'values': ['entropy', 'gini']}, 'max_features': {'default': None, 'type': 'str', 'values': [None, 'auto', 'log2', 'sqrt']}, 'min_impurity_decrease': {'default': 0.0, 'range': [0.0, 1000.0], 'type': 'float'}, 'min_samples_leaf': {'default': 1, 'range': [1, 100], 'type': 'int'}, 'min_samples_split': {'default': 2, 'range': [2, 100], 'type': 'int'}, 'min_weight_fraction_leaf': {'default': 0.0, 'range': [0.0, 0.5], 'type': 'float'}, 'n_estimators': {'default': 10, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindRFC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindRFC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindRFC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindRFC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.WindSGDC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.</code><code class="sig-name descname">WindSGDC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/wind.html#WindSGDC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.wind.html#btb.benchmark.challenges.wind.WindRFC" title="btb.benchmark.challenges.wind.WindRFC"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.wind.WindRFC</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.DATASET" title="btb.benchmark.challenges.WindSGDC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.ENCODE" title="btb.benchmark.challenges.WindSGDC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.MAKE_BINARY" title="btb.benchmark.challenges.WindSGDC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.MODEL_DEFAULTS" title="btb.benchmark.challenges.WindSGDC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.STRATIFIED" title="btb.benchmark.challenges.WindSGDC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.TARGET_COLUMN" title="btb.benchmark.challenges.WindSGDC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.WindSGDC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.METRIC" title="btb.benchmark.challenges.WindSGDC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.evaluate" title="btb.benchmark.challenges.WindSGDC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.get_tunable_hyperparameters" title="btb.benchmark.challenges.WindSGDC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.load_data" title="btb.benchmark.challenges.WindSGDC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.WindSGDC.MODEL" title="btb.benchmark.challenges.WindSGDC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.WindSGDC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'wind.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindSGDC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindSGDC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindSGDC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id15"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindSGDC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindSGDC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindSGDC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindSGDC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'class'</em><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.WindSGDC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'alpha': {'default': 0.0001, 'type': 'float', 'values': [0.0001, 1]}, 'loss': {'default': 'hinge', 'range': ['log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'], 'type': 'str'}, 'max_iter': {'default': 1000, 'type': 'int', 'values': [1, 5000]}, 'penalty': {'default': None, 'type': 'str', 'values': [None, 'l2', 'l1', 'elasticnet']}, 'shuffle': {'default': True, 'type': 'bool'}, 'tol': {'default': 0.001, 'type': 'float', 'values': [0.001, 1]}}</em><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindSGDC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindSGDC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.WindSGDC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.WindSGDC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="btb.benchmark.challenges.bohachevsky.html" class="btn btn-neutral float-right" title="btb.benchmark.challenges.bohachevsky module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="btb.benchmark.html" class="btn btn-neutral float-left" title="btb.benchmark package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, MIT Data To AI Lab

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>