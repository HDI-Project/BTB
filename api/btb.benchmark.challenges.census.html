

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>btb.benchmark.challenges.census module &mdash; BTB 0.3.6.dev2 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/dai-logo-white.ico"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> BTB
          

          
            
            <img src="../_static/dai-logo-white-200.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.3.6.dev2
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../install.html#stable-release">Stable Release</a></li>
<li class="toctree-l2"><a class="reference internal" href="../install.html#from-source">From source</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">User Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/tuners.html">Tuners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/selectors.html">Selectors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guides/btbsession.html">BTBSession</a></li>
</ul>
<p class="caption"><span class="caption-text">Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btb.session.html">Session Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="btb.tuning.html">Tuning Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="btb.tuning.acquisition.html">btb.tuning.acquisition package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.acquisition.base.html">btb.tuning.acquisition.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.acquisition.expected_improvement.html">btb.tuning.acquisition.expected_improvement module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.acquisition.predicted_score.html">btb.tuning.acquisition.predicted_score module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="btb.tuning.hyperparams.html">btb.tuning.hyperparams package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.hyperparams.base.html">btb.tuning.hyperparams.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.hyperparams.boolean.html">btb.tuning.hyperparams.boolean module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.hyperparams.categorical.html">btb.tuning.hyperparams.categorical module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.hyperparams.numerical.html">btb.tuning.hyperparams.numerical module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="btb.tuning.metamodels.html">btb.tuning.metamodels package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.metamodels.base.html">btb.tuning.metamodels.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.metamodels.gaussian_process.html">btb.tuning.metamodels.gaussian_process module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="btb.tuning.tuners.html">btb.tuning.tuners package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.tuners.base.html">btb.tuning.tuners.base module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.tuners.gaussian_process.html">btb.tuning.tuners.gaussian_process module</a></li>
<li class="toctree-l3"><a class="reference internal" href="btb.tuning.tuners.uniform.html">btb.tuning.tuners.uniform module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="btb.tuning.tunable.html">btb.tuning.tunable module</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="btb.selection.html">Selection Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="btb.selection.best.html">btb.selection.best module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btb.selection.custom_selector.html">btb.selection.custom_selector module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btb.selection.hierarchical.html">btb.selection.hierarchical module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btb.selection.pure.html">btb.selection.pure module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btb.selection.recent.html">btb.selection.recent module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btb.selection.selector.html">btb.selection.selector module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btb.selection.ucb1.html">btb.selection.ucb1 module</a></li>
<li class="toctree-l2"><a class="reference internal" href="btb.selection.uniform.html">btb.selection.uniform module</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Development Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#types-of-contributions">Types of Contributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#report-bugs">Report Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#fix-bugs">Fix Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#implement-features">Implement Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#write-documentation">Write Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#submit-feedback">Submit Feedback</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#unit-testing-guidelines">Unit Testing Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#tips">Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html#release-workflow">Release Workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../contributing.html#release-candidates">Release Candidates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../history.html">History</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id1">0.3.5 - 2020-01-21</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#internal-improvements">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#resolved-issues">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id2">0.3.4 - 2019-12-24</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-features">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id3">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id4">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id5">0.3.3 - 2019-12-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id6">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id7">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id8">0.3.2 - 2019-12-10</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id9">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id10">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id11">0.3.1 - 2019-11-25</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id12">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id13">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id14">0.3.0 - 2019-11-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-project-structure">New project structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#new-api">New API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id15">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id16">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id17">0.2.5</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#bug-fixes">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id18">0.2.4</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id19">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id20">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id21">0.2.3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id22">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id23">0.2.2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id24">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id25">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id26">0.2.1</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id27">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id28">0.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id29">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id30">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../history.html#id31">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id32">0.1.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../history.html#id33">0.1.1</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../authors.html">Credits</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">BTB</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>btb.benchmark.challenges.census module</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/HDI-Project/BTB/blob/master/docs/api/btb.benchmark.challenges.census.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-btb.benchmark.challenges.census">
<span id="btb-benchmark-challenges-census-module"></span><h1>btb.benchmark.challenges.census module<a class="headerlink" href="#module-btb.benchmark.challenges.census" title="Permalink to this headline">¶</a></h1>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC" title="btb.benchmark.challenges.census.CensusABC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CensusABC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC" title="btb.benchmark.challenges.census.CensusRFC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CensusRFC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p>The Census challenge is based on the <cite>Census Income Dataset</cite>.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC" title="btb.benchmark.challenges.census.CensusSGDC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">CensusSGDC</span></code></a>([model, dataset, target_column, …])</p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<dl class="class">
<dt id="btb.benchmark.challenges.census.CensusABC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.census.</code><code class="sig-name descname">CensusABC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/census.html#CensusABC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC" title="btb.benchmark.challenges.census.CensusRFC"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.census.CensusRFC</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.DATASET" title="btb.benchmark.challenges.census.CensusABC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.ENCODE" title="btb.benchmark.challenges.census.CensusABC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.MAKE_BINARY" title="btb.benchmark.challenges.census.CensusABC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.MODEL_DEFAULTS" title="btb.benchmark.challenges.census.CensusABC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.STRATIFIED" title="btb.benchmark.challenges.census.CensusABC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.TARGET_COLUMN" title="btb.benchmark.challenges.census.CensusABC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.census.CensusABC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.METRIC" title="btb.benchmark.challenges.census.CensusABC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.evaluate" title="btb.benchmark.challenges.census.CensusABC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.get_tunable_hyperparameters" title="btb.benchmark.challenges.census.CensusABC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.load_data" title="btb.benchmark.challenges.census.CensusABC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusABC.MODEL" title="btb.benchmark.challenges.census.CensusABC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>An AdaBoost classifier.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusABC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'census.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusABC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusABC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusABC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id1"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusABC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.weight_boosting.AdaBoostClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusABC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusABC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusABC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'income'</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusABC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'algorithm': {'default': 'SAMME.R', 'type': 'str', 'values': ['SAMME', 'SAMME.R']}, 'learning_rate': {'default': 1.0, 'range': [1.0, 10.0], 'type': 'float'}, 'n_estimators': {'default': 50, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusABC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusABC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusABC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusABC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.census.CensusRFC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.census.</code><code class="sig-name descname">CensusRFC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/census.html#CensusRFC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="btb.benchmark.challenges.challenge.html#btb.benchmark.challenges.challenge.MLChallenge" title="btb.benchmark.challenges.challenge.MLChallenge"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.challenge.MLChallenge</span></code></a></p>
<p>The Census challenge is based on the <cite>Census Income Dataset</cite>. The extraction
was done by Barry Becker from the 1994 Census Database. The prediction task
is to determine whether a person makes over 50.000 USD a year.</p>
<p>This dataset has been obtained from <a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/census+income">https://archive.ics.uci.edu/ml/datasets/census+income</a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.DATASET" title="btb.benchmark.challenges.census.CensusRFC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.ENCODE" title="btb.benchmark.challenges.census.CensusRFC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.MAKE_BINARY" title="btb.benchmark.challenges.census.CensusRFC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.MODEL_DEFAULTS" title="btb.benchmark.challenges.census.CensusRFC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.STRATIFIED" title="btb.benchmark.challenges.census.CensusRFC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.TARGET_COLUMN" title="btb.benchmark.challenges.census.CensusRFC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.census.CensusRFC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.METRIC" title="btb.benchmark.challenges.census.CensusRFC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.evaluate" title="btb.benchmark.challenges.census.CensusRFC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.get_tunable_hyperparameters" title="btb.benchmark.challenges.census.CensusRFC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.load_data" title="btb.benchmark.challenges.census.CensusRFC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC.MODEL" title="btb.benchmark.challenges.census.CensusRFC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>A random forest classifier.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusRFC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'census.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusRFC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusRFC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusRFC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusRFC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.ensemble.forest.RandomForestClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusRFC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusRFC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusRFC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'income'</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusRFC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'criterion': {'default': 'gini', 'type': 'str', 'values': ['entropy', 'gini']}, 'max_features': {'default': None, 'type': 'str', 'values': [None, 'auto', 'log2', 'sqrt']}, 'min_impurity_decrease': {'default': 0.0, 'range': [0.0, 1000.0], 'type': 'float'}, 'min_samples_leaf': {'default': 1, 'range': [1, 100], 'type': 'int'}, 'min_samples_split': {'default': 2, 'range': [2, 100], 'type': 'int'}, 'min_weight_fraction_leaf': {'default': 0.0, 'range': [0.0, 0.5], 'type': 'float'}, 'n_estimators': {'default': 10, 'range': [1, 500], 'type': 'int'}}</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusRFC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusRFC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusRFC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusRFC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btb.benchmark.challenges.census.CensusSGDC">
<em class="property">class </em><code class="sig-prename descclassname">btb.benchmark.challenges.census.</code><code class="sig-name descname">CensusSGDC</code><span class="sig-paren">(</span><em class="sig-param">model=None</em>, <em class="sig-param">dataset=None</em>, <em class="sig-param">target_column=None</em>, <em class="sig-param">encode=None</em>, <em class="sig-param">tunable_hyperparameters=None</em>, <em class="sig-param">metric=None</em>, <em class="sig-param">model_defaults=None</em>, <em class="sig-param">make_binary=None</em>, <em class="sig-param">stratified=None</em>, <em class="sig-param">cv_splits=5</em>, <em class="sig-param">cv_random_state=42</em>, <em class="sig-param">cv_shuffle=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/btb/benchmark/challenges/census.html#CensusSGDC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#btb.benchmark.challenges.census.CensusRFC" title="btb.benchmark.challenges.census.CensusRFC"><code class="xref py py-class docutils literal notranslate"><span class="pre">btb.benchmark.challenges.census.CensusRFC</span></code></a></p>
<p><strong>Attributes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.DATASET" title="btb.benchmark.challenges.census.CensusSGDC.DATASET"><code class="xref py py-obj docutils literal notranslate"><span class="pre">DATASET</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.ENCODE" title="btb.benchmark.challenges.census.CensusSGDC.ENCODE"><code class="xref py py-obj docutils literal notranslate"><span class="pre">ENCODE</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.MAKE_BINARY" title="btb.benchmark.challenges.census.CensusSGDC.MAKE_BINARY"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MAKE_BINARY</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.MODEL_DEFAULTS" title="btb.benchmark.challenges.census.CensusSGDC.MODEL_DEFAULTS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL_DEFAULTS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.STRATIFIED" title="btb.benchmark.challenges.census.CensusSGDC.STRATIFIED"><code class="xref py py-obj docutils literal notranslate"><span class="pre">STRATIFIED</span></code></a></p></td>
<td><p>bool(x) -&gt; bool</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.TARGET_COLUMN" title="btb.benchmark.challenges.census.CensusSGDC.TARGET_COLUMN"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TARGET_COLUMN</span></code></a></p></td>
<td><p>str(object=’’) -&gt; str</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.TUNABLE_HYPERPARAMETERS" title="btb.benchmark.challenges.census.CensusSGDC.TUNABLE_HYPERPARAMETERS"><code class="xref py py-obj docutils literal notranslate"><span class="pre">TUNABLE_HYPERPARAMETERS</span></code></a></p></td>
<td><p>dict() -&gt; new empty dictionary</p></td>
</tr>
</tbody>
</table>
<p><strong>Methods</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.METRIC" title="btb.benchmark.challenges.census.CensusSGDC.METRIC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">METRIC</span></code></a>(y_pred[, labels, pos_label, average, …])</p></td>
<td><p>Compute the F1 score, also known as balanced F-score or F-measure</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.evaluate" title="btb.benchmark.challenges.census.CensusSGDC.evaluate"><code class="xref py py-obj docutils literal notranslate"><span class="pre">evaluate</span></code></a>(**hyperparams)</p></td>
<td><p>Apply cross validation to hyperparameter combination.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.get_tunable_hyperparameters" title="btb.benchmark.challenges.census.CensusSGDC.get_tunable_hyperparameters"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_tunable_hyperparameters</span></code></a>()</p></td>
<td><p>Return a dictionary with hyperparameters to be tuned.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.load_data" title="btb.benchmark.challenges.census.CensusSGDC.load_data"><code class="xref py py-obj docutils literal notranslate"><span class="pre">load_data</span></code></a>()</p></td>
<td><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p></td>
</tr>
</tbody>
</table>
<p><strong>Classes</strong></p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#btb.benchmark.challenges.census.CensusSGDC.MODEL" title="btb.benchmark.challenges.census.CensusSGDC.MODEL"><code class="xref py py-obj docutils literal notranslate"><span class="pre">MODEL</span></code></a></p></td>
<td><p>Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</p></td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusSGDC.DATASET">
<code class="sig-name descname">DATASET</code><em class="property"> = 'census.csv'</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.DATASET" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusSGDC.ENCODE">
<code class="sig-name descname">ENCODE</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.ENCODE" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusSGDC.MAKE_BINARY">
<code class="sig-name descname">MAKE_BINARY</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.MAKE_BINARY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusSGDC.METRIC">
<code class="sig-name descname">METRIC</code><span class="sig-paren">(</span><em class="sig-param">y_pred</em>, <em class="sig-param">labels=None</em>, <em class="sig-param">pos_label=1</em>, <em class="sig-param">average='binary'</em>, <em class="sig-param">sample_weight=None</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.METRIC" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the F1 score, also known as balanced F-score or F-measure</p>
<p>The F1 score can be interpreted as a weighted average of the precision and
recall, where an F1 score reaches its best value at 1 and worst score at 0.
The relative contribution of precision and recall to the F1 score are
equal. The formula for the F1 score is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span>
</pre></div>
</div>
<p>In the multi-class and multi-label case, this is the average of
the F1 score of each class with weighting depending on the <code class="docutils literal notranslate"><span class="pre">average</span></code>
parameter.</p>
<p>Read more in the <span class="xref std std-ref">User Guide</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Ground truth (correct) target values.</p></li>
<li><p><strong>y_pred</strong> (<em>1d array-like</em><em>, or </em><em>label indicator array / sparse matrix</em>) – Estimated targets as returned by a classifier.</p></li>
<li><p><strong>labels</strong> (<em>list</em><em>, </em><em>optional</em>) – <p>The set of labels to include when <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code>, and their
order if <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">is</span> <span class="pre">None</span></code>. Labels present in the data can be
excluded, for example to calculate a multiclass average ignoring a
majority negative class, while labels not present in the data will
result in 0 components in a macro average. For multilabel targets,
labels are column indices. By default, all labels in <code class="docutils literal notranslate"><span class="pre">y_true</span></code> and
<code class="docutils literal notranslate"><span class="pre">y_pred</span></code> are used in sorted order.</p>
<div class="versionchanged">
<p><span class="versionmodified changed">Changed in version 0.17: </span>parameter <em>labels</em> improved for multiclass problem.</p>
</div>
</p></li>
<li><p><strong>pos_label</strong> (<em>str</em><em> or </em><em>int</em><em>, </em><em>1 by default</em>) – The class to report if <code class="docutils literal notranslate"><span class="pre">average='binary'</span></code> and the data is binary.
If the data are multiclass or multilabel, this will be ignored;
setting <code class="docutils literal notranslate"><span class="pre">labels=[pos_label]</span></code> and <code class="docutils literal notranslate"><span class="pre">average</span> <span class="pre">!=</span> <span class="pre">'binary'</span></code> will report
scores for that label only.</p></li>
<li><p><strong>average</strong> (<em>string</em><em>, </em><em>[</em><em>None</em><em>, </em><em>'binary'</em><em> (</em><em>default</em><em>)</em><em>, </em><em>'micro'</em><em>, </em><em>'macro'</em><em>, </em><em>'samples'</em><em>,                        </em><em>'weighted'</em><em>]</em>) – <p>This parameter is required for multiclass/multilabel targets.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the scores for each class are returned. Otherwise, this
determines the type of averaging performed on the data:</p>
<dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">'binary'</span></code>:</dt><dd><p>Only report results for the class specified by <code class="docutils literal notranslate"><span class="pre">pos_label</span></code>.
This is applicable only if targets (<code class="docutils literal notranslate"><span class="pre">y_{true,pred}</span></code>) are binary.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'micro'</span></code>:</dt><dd><p>Calculate metrics globally by counting the total true positives,
false negatives and false positives.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'macro'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their unweighted
mean.  This does not take label imbalance into account.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'weighted'</span></code>:</dt><dd><p>Calculate metrics for each label, and find their average weighted
by support (the number of true instances for each label). This
alters ‘macro’ to account for label imbalance; it can result in an
F-score that is not between precision and recall.</p>
</dd>
<dt><code class="docutils literal notranslate"><span class="pre">'samples'</span></code>:</dt><dd><p>Calculate metrics for each instance, and find their average (only
meaningful for multilabel classification where this differs from
<code class="xref py py-func docutils literal notranslate"><span class="pre">accuracy_score()</span></code>).</p>
</dd>
</dl>
</p></li>
<li><p><strong>sample_weight</strong> (<em>array-like of shape =</em><em> [</em><em>n_samples</em><em>]</em><em>, </em><em>optional</em>) – Sample weights.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><strong>f1_score</strong> – F1 score of the positive class in binary classification or weighted
average of the F1 scores of each class for the multiclass task.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float or array of float, shape = [n_unique_labels]</p>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fbeta_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">precision_recall_fscore_support()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">jaccard_score()</span></code>, <code class="xref py py-meth docutils literal notranslate"><span class="pre">multilabel_confusion_matrix()</span></code></p>
</div>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets">1</span></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a></p>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_true</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;macro&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;micro&#39;</span><span class="p">)</span>  
<span class="go">0.33...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="s1">&#39;weighted&#39;</span><span class="p">)</span>  
<span class="go">0.26...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="go">array([0.8, 0. , 0. ])</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<p>When <code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">positive</span> <span class="pre">==</span> <span class="pre">0</span></code> or
<code class="docutils literal notranslate"><span class="pre">true</span> <span class="pre">positive</span> <span class="pre">+</span> <span class="pre">false</span> <span class="pre">negative</span> <span class="pre">==</span> <span class="pre">0</span></code>, f-score returns 0 and raises
<code class="docutils literal notranslate"><span class="pre">UndefinedMetricWarning</span></code>.</p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusSGDC.MODEL">
<code class="sig-name descname">MODEL</code><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.MODEL" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.linear_model.stochastic_gradient.SGDClassifier</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusSGDC.MODEL_DEFAULTS">
<code class="sig-name descname">MODEL_DEFAULTS</code><em class="property"> = {'random_state': 0}</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.MODEL_DEFAULTS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusSGDC.STRATIFIED">
<code class="sig-name descname">STRATIFIED</code><em class="property"> = True</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.STRATIFIED" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusSGDC.TARGET_COLUMN">
<code class="sig-name descname">TARGET_COLUMN</code><em class="property"> = 'income'</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.TARGET_COLUMN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="attribute">
<dt id="btb.benchmark.challenges.census.CensusSGDC.TUNABLE_HYPERPARAMETERS">
<code class="sig-name descname">TUNABLE_HYPERPARAMETERS</code><em class="property"> = {'alpha': {'default': 0.0001, 'type': 'float', 'values': [0.0001, 1]}, 'loss': {'default': 'hinge', 'range': ['log', 'hinge', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'], 'type': 'str'}, 'max_iter': {'default': 1000, 'type': 'int', 'values': [1, 5000]}, 'penalty': {'default': None, 'type': 'str', 'values': [None, 'l2', 'l1', 'elasticnet']}, 'shuffle': {'default': True, 'type': 'bool'}, 'tol': {'default': 0.001, 'type': 'float', 'values': [0.001, 1]}}</em><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.TUNABLE_HYPERPARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusSGDC.evaluate">
<code class="sig-name descname">evaluate</code><span class="sig-paren">(</span><em class="sig-param">**hyperparams</em><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.evaluate" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply cross validation to hyperparameter combination.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>hyperparams</strong> (<em>dict</em>) – A combination of <code class="docutils literal notranslate"><span class="pre">self.tunable_hyperparams</span></code>.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returns the <code class="docutils literal notranslate"><span class="pre">mean</span></code> cross validated score.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>score (float)</p>
</dd>
</dl>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusSGDC.get_tunable_hyperparameters">
<code class="sig-name descname">get_tunable_hyperparameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.get_tunable_hyperparameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a dictionary with hyperparameters to be tuned.</p>
</dd></dl>

<dl class="method">
<dt id="btb.benchmark.challenges.census.CensusSGDC.load_data">
<code class="sig-name descname">load_data</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#btb.benchmark.challenges.census.CensusSGDC.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Load <code class="docutils literal notranslate"><span class="pre">X</span></code> and <code class="docutils literal notranslate"><span class="pre">y</span></code> over which to perform fit and evaluate.</p>
</dd></dl>

</dd></dl>

</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, MIT Data To AI Lab

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>