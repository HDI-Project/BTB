{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BTBSession\n",
    "\n",
    "## What is BTBSession\n",
    "\n",
    "BTBSession provides a simplified user interface to be able to search the best solution for your tuning problem by combining tuners and selectors with as little steps required as possible.\n",
    "\n",
    "## Creating a BTBSession\n",
    "\n",
    "We will guide you through the necessary steps to get started using BTBSession\n",
    "to select and tune the best model to solve a Machine Learning problem.\n",
    "\n",
    "In particular, in this example we will be using ``BTBSession`` to perform solve the [Wine](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data) classification problem by selecting between the\n",
    "`DecisionTreeClassifier` and the `SGDClassifier` models from [scikit-learn](\n",
    "https://scikit-learn.org/) while also searching for their best hyperparameter configuration.\n",
    "\n",
    "### Prepare a scoring function\n",
    "\n",
    "The first step in order to use the `BTBSession` class is to develop a scoring function.\n",
    "\n",
    "This is a Python function that, given a model name and a hyperparameter configuration,\n",
    "evaluates the performance of the model on your data and returns a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignoring warnings from external libraries that are irrelevant\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the dataset which we will use later on to evaluate the performance of our machine learning model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "dataset = load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a dictionary of our \"models\" with a given name as a key, this will help us when selecting to pick the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "models = {\n",
    "    'DTC': DecisionTreeClassifier,\n",
    "    'SGDC': SGDClassifier,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we can proceed to create our scoring function that will take as an input\n",
    "the model name (the key that we used previously) and the hyperparameter values. We will\n",
    "use the [cross_val_score](\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "that will use a `f1_score` as scorer.\n",
    "\n",
    "So our `scoring_function` will:\n",
    "1. Get the model using the name that we gave.\n",
    "2. Create that models instance with the given hyperparameter values.\n",
    "3. Generate scores using `corss_val_score`.\n",
    "4. Return the average score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def scoring_function(model_name, hyperparameter_values):\n",
    "    # choose the model\n",
    "    model_class = models[model_name]\n",
    "    \n",
    "    # instantiate the model\n",
    "    model_instance = model_class(**hyperparameter_values)\n",
    "    \n",
    "    # perform fit-score\n",
    "    scores = cross_val_score(\n",
    "        estimator=model_instance,\n",
    "        X=dataset.data,\n",
    "        y=dataset.target,\n",
    "        scoring=make_scorer(f1_score, average='macro')\n",
    "    )\n",
    "    \n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the tunable hyperparameters\n",
    "\n",
    "The second step is to define the hyperparameters that we want to tune for each model as `Tunables`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from btb.tuning import hyperparams as hp\n",
    "from btb.tuning import Tunable\n",
    "\n",
    "\n",
    "tunables = {\n",
    "    'DTC': Tunable({\n",
    "        'max_depth': hp.IntHyperParam(min=3, max=200),\n",
    "        'min_samples_split': hp.FloatHyperParam(min=0.01, max=1)\n",
    "    }),\n",
    "    'SGDC': Tunable({\n",
    "        'max_iter': hp.IntHyperParam(min=1, max=5000, default=1000),\n",
    "        'tol': hp.FloatHyperParam(min=1e-3, max=1, default=1e-3),\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create BTBSession instance\n",
    "\n",
    "Once you have defined a scoring function and the tunable hyperparameters specification of your\n",
    "models, you can create the instance of `btb.BTBSession`.\n",
    "\n",
    "BTBSession accepts the following arguments:\n",
    "\n",
    "- `tunables` (dict): Python dictionary that has as keys the name of the tunable and as value a dictionary with the tunable hyperparameters or an ``btb.tuning.tunable.Tunable`` instance.\n",
    "- `scorer` (callable object / function): A callable object or function with signature ``scorer(tunable_name, config)`` wich should return only a single value.\n",
    "- `tuner_class` (btb.tuning.tuner.BaseTuner): A tuner based on BTB ``BaseTuner`` class. This tuner will manage the new proposals. Defaults to ``btb.tuning.tuners.gaussian_process.GPTuner``\n",
    "- `selector_class` (btb.selection.selector.Selector): A selector based on BTB ``Selector`` class. This will determinate which one of the tunables is performing better, and which one to test next. Defaults to ``btb.selection.selectors.ucb1.UCB1``\n",
    "- `maximize` (bool): If ``True`` the scores are interpreted as bigger is better, if ``False`` then smaller is better, this should depend on the problem type (maximization or minimization). Defaults to ``True``.\n",
    "- `max_erors` (int): Amount of errors allowed for a tunable to not generate a score. Once this amount of errors is reached, the tunable will be removed from the list. Defaults to 1.\n",
    "- `verbose` (bool): If ``True`` a progress bar will be displayed for the ``run`` process.\n",
    "\n",
    "For now all you need to do is pass the tunable hyperparameters scpecification and the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from btb import BTBSession\n",
    "\n",
    "session = BTBSession(\n",
    "    tunables=tunables,\n",
    "    scorer=scoring_function,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using BTBSession\n",
    "\n",
    "### Run\n",
    "\n",
    "BTBSession works with it's main method called `run`. This method accepts as an argument\n",
    "the amount of tuning iterations to perform. By default this argument is `None` wich means\n",
    "that it will run until it's not stopped by the user or a `StopTuning` exception is raised.\n",
    "\n",
    "For now you can call the `run` method indicating how many tunable iterations you want the\n",
    "Session to perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5789aefba8ba41e6a14e83f5df19b5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=5), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_proposal = session.run(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the result\n",
    "\n",
    "The result will be a dictionary indicating the name of the best model that could be found\n",
    "and the hyperparameter configuration that was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '834d610fff74cae8a10e169c82346a0a',\n",
       " 'name': 'DTC',\n",
       " 'config': {'max_depth': 3, 'min_samples_split': 0.01},\n",
       " 'score': 0.8897699044250768}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The session object also contains this `best_proposal` as an attribute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '834d610fff74cae8a10e169c82346a0a',\n",
       " 'name': 'DTC',\n",
       " 'config': {'max_depth': 3, 'min_samples_split': 0.01},\n",
       " 'score': 0.8897699044250768}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.best_proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume session\n",
    "\n",
    "The session allows us to resume our tuning from the last iteration that we did. We can\n",
    "run for some more iterations and expect our score to be improved by calling the\n",
    "`run` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c3654d418442fe90b3a6b6357aa945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': '4a13e9f66e16e453fb9258ac59f27fa0',\n",
       " 'name': 'DTC',\n",
       " 'config': {'max_depth': 44, 'min_samples_split': 0.016971453207688683},\n",
       " 'score': 0.9076991973543698}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_proposal = session.run(20)\n",
    "\n",
    "best_proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe, this time, our score has improved after continuing our tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the best solution\n",
    "\n",
    "One we have found the best possible solution, we are ready to learn a model from our data in order to make predictions.\n",
    "To do this, we will have to retrieve from the `best_proposal` dict both the name and the configuration of the best solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name = best_proposal['name']\n",
    "hyperparameters = best_proposal['config']\n",
    "best_model_class = models[best_model_name]\n",
    "model_instance = best_model_class(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=44,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1,\n",
       "                       min_samples_split=0.016971453207688683,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_instance.fit(dataset.data, dataset.target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
