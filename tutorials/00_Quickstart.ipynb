{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "In this short tutorial we will guide you through the necessary steps to get started using BTB\n",
    "to select and tune the best model to solve a Machine Learning problem.\n",
    "\n",
    "In particular, in this example we will be using ``BTBSession`` to perform solve the [Wine](\n",
    "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data) classification problem\n",
    "by selecting between the `DecisionTreeClassifier` and the `SGDClassifier` models from\n",
    "[scikit-learn](https://scikit-learn.org/) while also searching for their best hyperparameter\n",
    "configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare a scoring function\n",
    "\n",
    "The first step in order to use the `BTBSession` class is to develop a scoring function.\n",
    "\n",
    "This is a Python function that, given a model name and a hyperparameter configuration,\n",
    "evaluates the performance of the model on your data and returns a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbsphinx": "hidden"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignoring warnings from external libraries that are irrelevant\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dataset = load_wine()\n",
    "\n",
    "models = {\n",
    "    'DTC': DecisionTreeClassifier,\n",
    "    'SGDC': SGDClassifier,\n",
    "}\n",
    "\n",
    "def scoring_function(model_name, hyperparameter_values):\n",
    "    model_class = models[model_name]\n",
    "    model_instance = model_class(**hyperparameter_values)\n",
    "    scores = cross_val_score(\n",
    "        estimator=model_instance,\n",
    "        X=dataset.data,\n",
    "        y=dataset.target,\n",
    "        scoring=make_scorer(f1_score, average='macro')\n",
    "    )\n",
    "    return scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the tunable hyperparameters\n",
    "\n",
    "The second step is to define the hyperparameters that we want to tune for each model as `Tunables`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from btb.tuning import Tunable\n",
    "from btb.tuning import hyperparams as hp\n",
    "\n",
    "tunables = {\n",
    "    'DTC': Tunable({\n",
    "        'max_depth': hp.IntHyperParam(min=3, max=200),\n",
    "        'min_samples_split': hp.FloatHyperParam(min=0.01, max=1)\n",
    "    }),\n",
    "    'SGDC': Tunable({\n",
    "        'max_iter': hp.IntHyperParam(min=1, max=5000, default=1000),\n",
    "        'tol': hp.FloatHyperParam(min=1e-3, max=1, default=1e-3),\n",
    "    })\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the searching process\n",
    "\n",
    "Once you have defined a scoring function and the tunable hyperparameters specification of your\n",
    "models, you can start the searching for the best model and hyperparameter configuration by using\n",
    "the `btb.BTBSession`.\n",
    "\n",
    "All you need to do is create an instance passing the tunable hyperparameters scpecification\n",
    "and the scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from btb import BTBSession\n",
    "\n",
    "session = BTBSession(\n",
    "    tunables=tunables,\n",
    "    scorer=scoring_function,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then call the `run` method indicating how many tunable iterations you want the Session to\n",
    "perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ff03b4d573c4ff1833e6b54dc66dfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_proposal = session.run(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be a dictionary indicating the name of the best model that could be found\n",
    "and the hyperparameter configuration that was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'e47c13afa40a10d55da91a13add6e142',\n",
       " 'name': 'DTC',\n",
       " 'config': {'max_depth': 3, 'min_samples_split': 0.1445639630277333},\n",
       " 'score': 0.9127678612465631}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_proposal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
