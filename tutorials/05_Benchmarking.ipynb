{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**BTB** provides a benchmarking framework that allows users and developers to evaluate the\n",
    "performance of the BTB Tuners for Machine Learning Hyperparameter Tuning on hundreds of real world\n",
    "classification problem and classical mathematical optimization problems.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "In order to use this `benchmarking` module, you will have to fork the\n",
    "**BTB** repository\n",
    "and install it from source. You can visit our\n",
    "[Get Started](https://hdi-project.github.io/BTB/contributing.html#get-started)\n",
    "tutorial and follow until step 4, which explains how to clone and install the repository\n",
    "from it's source.\n",
    "\n",
    "### The Benchmarking process\n",
    "\n",
    "The Benchmarking BTB process has two main concepts.\n",
    "\n",
    "#### Challenges\n",
    "\n",
    "A Challenge of the BTB Benchmarking framework is a Python class which has a method that produces a\n",
    "score that can be optimized by tuning a set of hyperparameters.\n",
    "\n",
    "#### Tuning Functions\n",
    "\n",
    "In the context of the BTB Benchmarking, `Tuning Functions` are python functions that, given a scoring\n",
    "function and its tunable hyperparameters, try to search for the ideal hyperparameter values within\n",
    "a given number of iterations.\n",
    "\n",
    "If you want to add a tuner, you could follow the specific signature a tuning function has:\n",
    "\n",
    "```python3\n",
    "def tuning_function(\n",
    "    scoring_function: callable,\n",
    "    tunable_hyperparameters: dict,\n",
    "    iterations: int) -> score: float\n",
    "```\n",
    "\n",
    "\n",
    "### Running the Benchmarking\n",
    "\n",
    "The user API for the BTB Benchmarking is the `btb_benchmark.main.run_benchmark` function.\n",
    "\n",
    "The `run_benchmark` function has the following arguments:\n",
    "\n",
    "- `tuners`: list of tuners that will be benchmarked.\n",
    "- `challenge_types`: list of types of challenges that will be used for benchmark (optional).\n",
    "- `challenges`: list of names of challenges that will be benchmarked (optional).\n",
    "- `sample`: if specified, run the benchmark on a subset of the available challenges of the given size (optional).\n",
    "- `iterations`: the number of tuning iterations to perform per challenge and tuner.\n",
    "- `output_path`: If given, store the benchmark results in the given path as a CSV file.\n",
    "\n",
    "## Example\n",
    "\n",
    "In this example we will create a `tuning_function` for a `GPEiTuner`,\n",
    "then we will import some `challenges` and we will run a benchmark\n",
    "over our `GPEiTuner` against a `Uniform` tuner already implemented\n",
    "in `btb.benchmark`.\n",
    "\n",
    "To start, we will import our `GPEiTuner` and we will create the\n",
    "tuning function that instantiates a `Tunable` that will be used\n",
    "by our `tuner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from btb.tuning import GPEiTuner, Tunable\n",
    "\n",
    "def tuning_function(scoring_function, tunable_hp, iterations):\n",
    "    tunable = Tunable.from_dict(tunable_hp)\n",
    "    tuner = GPEiTuner(tunable)\n",
    "    \n",
    "    best_score = -np.inf\n",
    "    \n",
    "    for _ in range(iterations):\n",
    "        proposal = tuner.propose()\n",
    "        score = scoring_function(**proposal)\n",
    "        tuner.record(proposal, score)\n",
    "        \n",
    "        best_score = max(score, best_score)\n",
    "        \n",
    "    return best_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our `tuning_function` against a set of mathematical\n",
    "optimization problems and see how it performs by giving as arguments\n",
    "`tuners=tuning_function` and `challenge_types=math`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from btb_benchmark import run_benchmark\n",
    "\n",
    "scores = run_benchmark(\n",
    "    tuners=tuning_function, challenge_types='math', iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tuning_function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bohachevsky()</th>\n",
       "      <td>-489.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Branin()</th>\n",
       "      <td>-7.961228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosenbrock()</th>\n",
       "      <td>-16901.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tuning_function\n",
       "Bohachevsky()      -489.600000\n",
       "Branin()             -7.961228\n",
       "Rosenbrock()     -16901.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's score our tuning function against an already implemented\n",
    "`BTB.Uniform` tuning function, by creating a list of the tuners we \n",
    "want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTB.UniformTuner</th>\n",
       "      <th>tuning_function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bohachevsky()</th>\n",
       "      <td>-99.600000</td>\n",
       "      <td>-1.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Branin()</th>\n",
       "      <td>-0.674776</td>\n",
       "      <td>-0.402413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rosenbrock()</th>\n",
       "      <td>-6404.000000</td>\n",
       "      <td>-436.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               BTB.UniformTuner  tuning_function\n",
       "Bohachevsky()        -99.600000        -1.600000\n",
       "Branin()              -0.674776        -0.402413\n",
       "Rosenbrock()       -6404.000000      -436.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuners = [tuning_function, 'BTB.UniformTuner']\n",
    "\n",
    "scores = run_benchmark(\n",
    "    tuners=tuners,\n",
    "    challenge_types='math',\n",
    "    iterations=50\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the challenges that we would like to run,\n",
    "so let's just run our benchmarking against the `Rosenbrock` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTB.UniformTuner</th>\n",
       "      <th>tuning_function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Rosenbrock()</th>\n",
       "      <td>-400</td>\n",
       "      <td>-64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              BTB.UniformTuner  tuning_function\n",
       "Rosenbrock()              -400              -64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = run_benchmark(\n",
    "    tuners=tuners,\n",
    "    challenges='rosenbrock',\n",
    "    iterations=100\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also instantiate a `challenge` and use it, like we did with\n",
    "our `tuning_function`. Let's import a Machine Learning challenge and\n",
    "instantiate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from btb_benchmark.challenges import SGDChallenge\n",
    "\n",
    "challenge = SGDChallenge('pollution_1')\n",
    "scores = run_benchmark(\n",
    "    tuners=tuners,\n",
    "    challenges=challenge,\n",
    "    iterations=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BTB.UniformTuner</th>\n",
       "      <th>tuning_function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SGDChallenge('pollution_1')</th>\n",
       "      <td>0.646401</td>\n",
       "      <td>0.605132</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             BTB.UniformTuner  tuning_function\n",
       "SGDChallenge('pollution_1')          0.646401         0.605132"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have created a `tuner` based on `btb.tuning.tuners.base.BaseTuner` you can\n",
    "directly pass it as a `tuenrs` argument and it will create a `TuningFunction` similar\n",
    "to the one created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GPEiTuner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SGDChallenge('pollution_1')</th>\n",
       "      <td>0.643693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             GPEiTuner\n",
       "SGDChallenge('pollution_1')   0.643693"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_benchmark(tuners=GPEiTuner, challenges=challenge, iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
