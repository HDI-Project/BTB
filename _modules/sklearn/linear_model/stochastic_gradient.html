

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>sklearn.linear_model.stochastic_gradient &mdash; BTB 0.3.6.dev0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../_static/dai-logo-white.ico"/>
  
  
  

  
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
        <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home"> BTB
          

          
            
            <img src="../../../_static/dai-logo-white-200.png" class="logo" alt="Logo"/>
          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html#install">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#requirements">Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-using-pip">Install using Pip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-from-source">Install from Source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#install-for-development">Install for Development</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../readme.html#quickstart">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#tuners">Tuners</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../readme.html#selectors">Selectors</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#what-s-next">Whatâ€™s next?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../readme.html#citing-btb">Citing BTB</a></li>
</ul>
</li>
</ul>
<p class="caption"><span class="caption-text">Resources</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../api/btb.html">API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../api/btb.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.benchmark.html">btb.benchmark package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.benchmark.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.benchmark.html#module-btb.benchmark">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.selection.html">btb.selection package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.selection.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.selection.html#module-btb.selection">Module contents</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.tuning.html">btb.tuning package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#subpackages">Subpackages</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../api/btb.tuning.html#module-btb.tuning">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/btb.html#submodules">Submodules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../api/btb.session.html">btb.session module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../api/btb.html#module-btb">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#types-of-contributions">Types of Contributions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#report-bugs">Report Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#fix-bugs">Fix Bugs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#implement-features">Implement Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#write-documentation">Write Documentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#submit-feedback">Submit Feedback</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#get-started">Get Started!</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#pull-request-guidelines">Pull Request Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#unit-testing-guidelines">Unit Testing Guidelines</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#tips">Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../contributing.html#release-workflow">Release Workflow</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../contributing.html#release-candidates">Release Candidates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../authors.html">Credits</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../history.html">History</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id1">0.3.5 - 2020-01-21</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#internal-improvements">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#resolved-issues">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id2">0.3.4 - 2019-12-24</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#new-features">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id3">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id4">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id5">0.3.3 - 2019-12-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id6">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id7">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id8">0.3.2 - 2019-12-10</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id9">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id10">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id11">0.3.1 - 2019-11-25</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id12">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id13">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id14">0.3.0 - 2019-11-11</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#new-project-structure">New project structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#new-api">New API</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id15">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id16">Resolved Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id17">0.2.5</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#bug-fixes">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id18">0.2.4</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id19">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id20">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id21">0.2.3</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id22">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id23">0.2.2</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id24">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id25">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id26">0.2.1</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id27">Bug fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id28">0.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id29">New Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id30">Internal Improvements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../history.html#id31">Bug Fixes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id32">0.1.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../history.html#id33">0.1.1</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">BTB</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../../index.html">Module code</a> &raquo;</li>
        
      <li>sklearn.linear_model.stochastic_gradient</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <h1>Source code for sklearn.linear_model.stochastic_gradient</h1><div class="highlight"><pre>
<span></span><span class="c1"># Authors: Peter Prettenhofer &lt;peter.prettenhofer@gmail.com&gt; (main author)</span>
<span class="c1">#          Mathieu Blondel (partial_fit support)</span>
<span class="c1">#</span>
<span class="c1"># License: BSD 3 clause</span>
<span class="sd">&quot;&quot;&quot;Classification and regression using Stochastic Gradient Descent (SGD).&quot;&quot;&quot;</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABCMeta</span><span class="p">,</span> <span class="n">abstractmethod</span>

<span class="kn">from</span> <span class="nn">..utils._joblib</span> <span class="kn">import</span> <span class="n">Parallel</span><span class="p">,</span> <span class="n">delayed</span>

<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">clone</span><span class="p">,</span> <span class="n">is_classifier</span>
<span class="kn">from</span> <span class="nn">.base</span> <span class="kn">import</span> <span class="n">LinearClassifierMixin</span><span class="p">,</span> <span class="n">SparseCoefMixin</span>
<span class="kn">from</span> <span class="nn">.base</span> <span class="kn">import</span> <span class="n">make_dataset</span>
<span class="kn">from</span> <span class="nn">..base</span> <span class="kn">import</span> <span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">check_array</span><span class="p">,</span> <span class="n">check_random_state</span><span class="p">,</span> <span class="n">check_X_y</span>
<span class="kn">from</span> <span class="nn">..utils.extmath</span> <span class="kn">import</span> <span class="n">safe_sparse_dot</span>
<span class="kn">from</span> <span class="nn">..utils.multiclass</span> <span class="kn">import</span> <span class="n">_check_partial_fit_first_call</span>
<span class="kn">from</span> <span class="nn">..utils.validation</span> <span class="kn">import</span> <span class="n">check_is_fitted</span>
<span class="kn">from</span> <span class="nn">..exceptions</span> <span class="kn">import</span> <span class="n">ConvergenceWarning</span>
<span class="kn">from</span> <span class="nn">..model_selection</span> <span class="kn">import</span> <span class="n">StratifiedShuffleSplit</span><span class="p">,</span> <span class="n">ShuffleSplit</span>

<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">plain_sgd</span><span class="p">,</span> <span class="n">average_sgd</span>
<span class="kn">from</span> <span class="nn">..utils</span> <span class="kn">import</span> <span class="n">compute_class_weight</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">Hinge</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">SquaredHinge</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">Log</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">ModifiedHuber</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">SquaredLoss</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">Huber</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">EpsilonInsensitive</span>
<span class="kn">from</span> <span class="nn">.sgd_fast</span> <span class="kn">import</span> <span class="n">SquaredEpsilonInsensitive</span>
<span class="kn">from</span> <span class="nn">..utils.fixes</span> <span class="kn">import</span> <span class="n">_joblib_parallel_args</span>

<span class="n">LEARNING_RATE_TYPES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;constant&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;optimal&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;invscaling&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
                       <span class="s2">&quot;adaptive&quot;</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;pa1&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span> <span class="s2">&quot;pa2&quot;</span><span class="p">:</span> <span class="mi">6</span><span class="p">}</span>

<span class="n">PENALTY_TYPES</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;none&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;l2&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">&quot;l1&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;elasticnet&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">}</span>

<span class="n">DEFAULT_EPSILON</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="c1"># Default value of ``epsilon`` parameter.</span>

<span class="n">MAX_INT</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span>


<span class="k">class</span> <span class="nc">_ValidationScoreCallback</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Callback for early stopping based on validation score&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">estimator</span><span class="p">,</span> <span class="n">X_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">,</span> <span class="n">sample_weight_val</span><span class="p">,</span>
                 <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">estimator</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># to pass check_is_fitted</span>
        <span class="k">if</span> <span class="n">classes</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="n">classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X_val</span> <span class="o">=</span> <span class="n">X_val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y_val</span> <span class="o">=</span> <span class="n">y_val</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight_val</span> <span class="o">=</span> <span class="n">sample_weight_val</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">):</span>
        <span class="n">est</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimator</span>
        <span class="n">est</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">est</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">intercept</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">est</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight_val</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">BaseSGD</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">SparseCoefMixin</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for SGD classification and regression.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;optimal&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">power_t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">penalty</span> <span class="o">=</span> <span class="n">penalty</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">C</span> <span class="o">=</span> <span class="n">C</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">=</span> <span class="n">l1_ratio</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta0</span> <span class="o">=</span> <span class="n">eta0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">power_t</span> <span class="o">=</span> <span class="n">power_t</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">early_stopping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validation_fraction</span> <span class="o">=</span> <span class="n">validation_fraction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="o">=</span> <span class="n">n_iter_no_change</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="o">=</span> <span class="n">warm_start</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">=</span> <span class="n">average</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">=</span> <span class="n">max_iter</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">=</span> <span class="n">tol</span>
        <span class="c1"># current tests expect init to do parameter validation</span>
        <span class="c1"># but we are not allowed to set attributes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">(</span><span class="n">set_max_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">(</span><span class="n">set_max_iter</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit model.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">_validate_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">set_max_iter</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">for_partial_fit</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Validate input params. &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;shuffle must be either True or False&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span> <span class="nb">bool</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;early_stopping must be either True or False&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span> <span class="ow">and</span> <span class="n">for_partial_fit</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;early_stopping should be False with partial_fit&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_iter must be &gt; zero. Got </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;l1_ratio must be in [0, 1]&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha must be &gt;= 0&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;n_iter_no_change must be &gt;= 1&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.0</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_fraction</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;validation_fraction must be in ]0, 1[&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;constant&quot;</span><span class="p">,</span> <span class="s2">&quot;invscaling&quot;</span><span class="p">,</span> <span class="s2">&quot;adaptive&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta0</span> <span class="o">&lt;=</span> <span class="mf">0.0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;eta0 must be &gt; 0&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">==</span> <span class="s2">&quot;optimal&quot;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;alpha must be &gt; 0 since &quot;</span>
                             <span class="s2">&quot;learning_rate is &#39;optimal&#39;. alpha is used &quot;</span>
                             <span class="s2">&quot;to compute the optimal learning rate.&quot;</span><span class="p">)</span>

        <span class="c1"># raises ValueError if not registered</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_get_penalty_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_get_learning_rate_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_functions</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The loss </span><span class="si">%s</span><span class="s2"> is not supported. &quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">set_max_iter</span><span class="p">:</span>
            <span class="k">return</span>

    <span class="k">def</span> <span class="nf">_get_loss_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get concrete ``LossFunction`` object for str ``loss``. &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">loss_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_functions</span><span class="p">[</span><span class="n">loss</span><span class="p">]</span>
            <span class="n">loss_class</span><span class="p">,</span> <span class="n">args</span> <span class="o">=</span> <span class="n">loss_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">loss_</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
            <span class="k">if</span> <span class="n">loss</span> <span class="ow">in</span> <span class="p">(</span><span class="s1">&#39;huber&#39;</span><span class="p">,</span> <span class="s1">&#39;epsilon_insensitive&#39;</span><span class="p">,</span>
                        <span class="s1">&#39;squared_epsilon_insensitive&#39;</span><span class="p">):</span>
                <span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">,</span> <span class="p">)</span>
            <span class="k">return</span> <span class="n">loss_class</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The loss </span><span class="si">%s</span><span class="s2"> is not supported. &quot;</span> <span class="o">%</span> <span class="n">loss</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_learning_rate_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">LEARNING_RATE_TYPES</span><span class="p">[</span><span class="n">learning_rate</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;learning rate </span><span class="si">%s</span><span class="s2"> &quot;</span>
                             <span class="s2">&quot;is not supported. &quot;</span> <span class="o">%</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_penalty_type</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">penalty</span><span class="p">):</span>
        <span class="n">penalty</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">penalty</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">PENALTY_TYPES</span><span class="p">[</span><span class="n">penalty</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Penalty </span><span class="si">%s</span><span class="s2"> is not supported. &quot;</span> <span class="o">%</span> <span class="n">penalty</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_validate_sample_weight</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set the sample weight array.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># uniform sample weights</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># user-provided array</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                       <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">n_samples</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Shapes of X and sample_weight do not match.&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sample_weight</span>

    <span class="k">def</span> <span class="nf">_allocate_parameter_mem</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Allocate mem for parameters; initialize if provided.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="c1"># allocate coef_ for multi-class</span>
            <span class="k">if</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">coef_init</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">coef_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided ``coef_`` does not match &quot;</span>
                                     <span class="s2">&quot;dataset. &quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">coef_init</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span><span class="p">),</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

            <span class="c1"># allocate intercept_ for multi-class</span>
            <span class="k">if</span> <span class="n">intercept_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">intercept_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">intercept_init</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">intercept_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided intercept_init &quot;</span>
                                     <span class="s2">&quot;does not match dataset.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">intercept_init</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                           <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># allocate coef_ for binary problem</span>
            <span class="k">if</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">coef_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                       <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="n">coef_init</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">coef_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided coef_init does not &quot;</span>
                                     <span class="s2">&quot;match dataset.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">coef_init</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span>
                                      <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                      <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

            <span class="c1"># allocate intercept_ for binary problem</span>
            <span class="k">if</span> <span class="n">intercept_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">intercept_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">intercept_init</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">intercept_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)</span> <span class="ow">and</span> <span class="n">intercept_init</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="p">():</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Provided intercept_init &quot;</span>
                                     <span class="s2">&quot;does not match dataset.&quot;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">intercept_init</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

        <span class="c1"># initialize average parameters</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                          <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                               <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                               <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_make_validation_split</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Split the dataset between training set and validation set.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        y : array, shape (n_samples, )</span>
<span class="sd">            Target values.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        validation_mask : array, shape (n_samples, )</span>
<span class="sd">            Equal to 1 on the validation set, 0 on the training set.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">n_samples</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">validation_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint8</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">:</span>
            <span class="c1"># use the full set for training, with an empty validation set</span>
            <span class="k">return</span> <span class="n">validation_mask</span>

        <span class="k">if</span> <span class="n">is_classifier</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="n">splitter_type</span> <span class="o">=</span> <span class="n">StratifiedShuffleSplit</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">splitter_type</span> <span class="o">=</span> <span class="n">ShuffleSplit</span>
        <span class="n">cv</span> <span class="o">=</span> <span class="n">splitter_type</span><span class="p">(</span><span class="n">test_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">validation_fraction</span><span class="p">,</span>
                           <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">idx_train</span><span class="p">,</span> <span class="n">idx_val</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">y</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">idx_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">idx_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Splitting </span><span class="si">%d</span><span class="s2"> samples into a train set and a validation set &quot;</span>
                <span class="s2">&quot;with validation_fraction=</span><span class="si">%r</span><span class="s2"> led to an empty set (</span><span class="si">%d</span><span class="s2"> and </span><span class="si">%d</span><span class="s2"> &quot;</span>
                <span class="s2">&quot;samples). Please either change validation_fraction, increase &quot;</span>
                <span class="s2">&quot;number of samples, or disable early_stopping.&quot;</span>
                <span class="o">%</span> <span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">validation_fraction</span><span class="p">,</span> <span class="n">idx_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                   <span class="n">idx_val</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>

        <span class="n">validation_mask</span><span class="p">[</span><span class="n">idx_val</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">return</span> <span class="n">validation_mask</span>

    <span class="k">def</span> <span class="nf">_make_validation_score_cb</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">validation_mask</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                  <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span>

        <span class="k">return</span> <span class="n">_ValidationScoreCallback</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">[</span><span class="n">validation_mask</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">validation_mask</span><span class="p">],</span>
            <span class="n">sample_weight</span><span class="p">[</span><span class="n">validation_mask</span><span class="p">],</span> <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_prepare_fit_binary</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialization for fit_binary.</span>

<span class="sd">    Returns y, coef, intercept, average_coef, average_intercept.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
    <span class="n">y_i</span><span class="p">[</span><span class="n">y</span> <span class="o">!=</span> <span class="n">est</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1.0</span>
    <span class="n">average_intercept</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">average_coef</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">est</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">standard_coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="n">average_coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">average_coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
            <span class="n">average_intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">est</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">standard_coef_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">average_coef</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">average_coef_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
            <span class="n">average_intercept</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">average_coef</span><span class="p">,</span> <span class="n">average_intercept</span>


<span class="k">def</span> <span class="nf">fit_binary</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span>
               <span class="n">pos_weight</span><span class="p">,</span> <span class="n">neg_weight</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">validation_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
               <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit a single binary classifier.</span>

<span class="sd">    The i&#39;th class is considered the &quot;positive&quot; class.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    est : Estimator object</span>
<span class="sd">        The estimator to fit</span>

<span class="sd">    i : int</span>
<span class="sd">        Index of the positive class</span>

<span class="sd">    X : numpy array or sparse matrix of shape [n_samples,n_features]</span>
<span class="sd">        Training data</span>

<span class="sd">    y : numpy array of shape [n_samples, ]</span>
<span class="sd">        Target values</span>

<span class="sd">    alpha : float</span>
<span class="sd">        The regularization parameter</span>

<span class="sd">    C : float</span>
<span class="sd">        Maximum step size for passive aggressive</span>

<span class="sd">    learning_rate : string</span>
<span class="sd">        The learning rate. Accepted values are &#39;constant&#39;, &#39;optimal&#39;,</span>
<span class="sd">        &#39;invscaling&#39;, &#39;pa1&#39; and &#39;pa2&#39;.</span>

<span class="sd">    max_iter : int</span>
<span class="sd">        The maximum number of iterations (epochs)</span>

<span class="sd">    pos_weight : float</span>
<span class="sd">        The weight of the positive class</span>

<span class="sd">    neg_weight : float</span>
<span class="sd">        The weight of the negative class</span>

<span class="sd">    sample_weight : numpy array of shape [n_samples, ]</span>
<span class="sd">        The weight of each sample</span>

<span class="sd">    validation_mask : numpy array of shape [n_samples, ] or None</span>
<span class="sd">        Precomputed validation mask in case _fit_binary is called in the</span>
<span class="sd">        context of a one-vs-rest reduction.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        If int, random_state is the seed used by the random number generator;</span>
<span class="sd">        If RandomState instance, random_state is the random number generator;</span>
<span class="sd">        If None, the random number generator is the RandomState instance used</span>
<span class="sd">        by `np.random`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># if average is not true, average_coef, and average_intercept will be</span>
    <span class="c1"># unused</span>
    <span class="n">y_i</span><span class="p">,</span> <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">average_coef</span><span class="p">,</span> <span class="n">average_intercept</span> <span class="o">=</span> \
        <span class="n">_prepare_fit_binary</span><span class="p">(</span><span class="n">est</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">y_i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">dataset</span><span class="p">,</span> <span class="n">intercept_decay</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">)</span>

    <span class="n">penalty_type</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">_get_penalty_type</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>
    <span class="n">learning_rate_type</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">_get_learning_rate_type</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">validation_mask</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">validation_mask</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">_make_validation_split</span><span class="p">(</span><span class="n">y_i</span><span class="p">)</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">y_i</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">validation_score_cb</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">_make_validation_score_cb</span><span class="p">(</span>
        <span class="n">validation_mask</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y_i</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>

    <span class="c1"># numpy mtrand expects a C long which is a signed 32 bit integer under</span>
    <span class="c1"># Windows</span>
    <span class="n">seed</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">)</span>

    <span class="n">tol</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">tol</span> <span class="k">if</span> <span class="n">est</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">est</span><span class="o">.</span><span class="n">average</span><span class="p">:</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">plain_sgd</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">loss_function_</span><span class="p">,</span>
                           <span class="n">penalty_type</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
                           <span class="n">dataset</span><span class="p">,</span> <span class="n">validation_mask</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
                           <span class="n">validation_score_cb</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">),</span>
                           <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">),</span>
                           <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">shuffle</span><span class="p">),</span> <span class="n">seed</span><span class="p">,</span>
                           <span class="n">pos_weight</span><span class="p">,</span> <span class="n">neg_weight</span><span class="p">,</span>
                           <span class="n">learning_rate_type</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">eta0</span><span class="p">,</span>
                           <span class="n">est</span><span class="o">.</span><span class="n">power_t</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span> <span class="n">intercept_decay</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">standard_coef</span><span class="p">,</span> <span class="n">standard_intercept</span><span class="p">,</span> <span class="n">average_coef</span><span class="p">,</span> <span class="n">average_intercept</span><span class="p">,</span> \
            <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">average_sgd</span><span class="p">(</span><span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">average_coef</span><span class="p">,</span>
                                  <span class="n">average_intercept</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">loss_function_</span><span class="p">,</span>
                                  <span class="n">penalty_type</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
                                  <span class="n">dataset</span><span class="p">,</span> <span class="n">validation_mask</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
                                  <span class="n">validation_score_cb</span><span class="p">,</span>
                                  <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">),</span> <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span>
                                  <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span>
                                  <span class="nb">int</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">shuffle</span><span class="p">),</span> <span class="n">seed</span><span class="p">,</span> <span class="n">pos_weight</span><span class="p">,</span>
                                  <span class="n">neg_weight</span><span class="p">,</span> <span class="n">learning_rate_type</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">eta0</span><span class="p">,</span>
                                  <span class="n">est</span><span class="o">.</span><span class="n">power_t</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span> <span class="n">intercept_decay</span><span class="p">,</span>
                                  <span class="n">est</span><span class="o">.</span><span class="n">average</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">est</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">est</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">average_intercept</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">est</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">average_intercept</span>

        <span class="n">result</span> <span class="o">=</span> <span class="n">standard_coef</span><span class="p">,</span> <span class="n">standard_intercept</span><span class="p">,</span> <span class="n">n_iter_</span>

    <span class="k">return</span> <span class="n">result</span>


<span class="k">class</span> <span class="nc">BaseSGDClassifier</span><span class="p">(</span><span class="n">BaseSGD</span><span class="p">,</span> <span class="n">LinearClassifierMixin</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ABCMeta</span><span class="p">):</span>

    <span class="n">loss_functions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;hinge&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Hinge</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="s2">&quot;squared_hinge&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredHinge</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">),</span>
        <span class="s2">&quot;perceptron&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Hinge</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">),</span>
        <span class="s2">&quot;log&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Log</span><span class="p">,</span> <span class="p">),</span>
        <span class="s2">&quot;modified_huber&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">ModifiedHuber</span><span class="p">,</span> <span class="p">),</span>
        <span class="s2">&quot;squared_loss&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredLoss</span><span class="p">,</span> <span class="p">),</span>
        <span class="s2">&quot;huber&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Huber</span><span class="p">,</span> <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
        <span class="s2">&quot;epsilon_insensitive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">EpsilonInsensitive</span><span class="p">,</span> <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
        <span class="s2">&quot;squared_epsilon_insensitive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredEpsilonInsensitive</span><span class="p">,</span>
                                        <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">DEFAULT_EPSILON</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;optimal&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">power_t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">,</span>
            <span class="n">power_t</span><span class="o">=</span><span class="n">power_t</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="n">validation_fraction</span><span class="o">=</span><span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">n_iter_no_change</span><span class="o">=</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="o">=</span> <span class="n">class_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span> <span class="o">=</span> <span class="n">n_jobs</span>

    <span class="k">def</span> <span class="nf">_partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
                     <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span>
                     <span class="n">classes</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                     <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>
                         <span class="n">accept_large_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="n">_check_partial_fit_first_call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">classes</span><span class="p">)</span>

        <span class="n">n_classes</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="c1"># Allocate datastructures from input arguments</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_expanded_class_weight</span> <span class="o">=</span> <span class="n">compute_class_weight</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span>
                                                           <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_allocate_parameter_mem</span><span class="p">(</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span>
                                         <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">n_features</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of features </span><span class="si">%d</span><span class="s2"> does not match previous &quot;</span>
                             <span class="s2">&quot;data </span><span class="si">%d</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_function_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss_function</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;t_&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="c1"># delegate to concrete training procedure</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_multiclass</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                 <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_fit_binary</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">,</span>
                             <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                             <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                             <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;The number of classes has to be greater than one;&quot;</span>
                <span class="s2">&quot; got </span><span class="si">%d</span><span class="s2"> class&quot;</span> <span class="o">%</span> <span class="n">n_classes</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;classes_&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;csr&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">,</span>
                         <span class="n">accept_large_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># labels can be encoded as float, int, or string literals</span>
        <span class="c1"># np.unique sorts in asc order; largest class id is positive class</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="k">if</span> <span class="n">intercept_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">intercept_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Clear iteration count for multiple call to fit.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span>
                          <span class="n">classes</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&gt;</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Maximum number of iteration reached before &quot;</span>
                          <span class="s2">&quot;convergence. Consider increasing max_iter to &quot;</span>
                          <span class="s2">&quot;improve the fit.&quot;</span><span class="p">,</span>
                          <span class="n">ConvergenceWarning</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">_fit_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit a binary classifier on X and y. &quot;&quot;&quot;</span>
        <span class="n">coef</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">n_iter_</span> <span class="o">=</span> <span class="n">fit_binary</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
                                              <span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">,</span>
                                              <span class="bp">self</span><span class="o">.</span><span class="n">_expanded_class_weight</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
                                              <span class="bp">self</span><span class="o">.</span><span class="n">_expanded_class_weight</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                                              <span class="n">sample_weight</span><span class="p">,</span>
                                              <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">+=</span> <span class="n">n_iter_</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">n_iter_</span>

        <span class="c1"># need to be 2d</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">intercept</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="n">coef</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="c1"># intercept is a float, need to convert it to an array of length 1</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">intercept</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit_multiclass</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                        <span class="n">sample_weight</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit a multi-class classifier by combining binary classifiers</span>

<span class="sd">        Each binary classifier predicts one class versus all others. This</span>
<span class="sd">        strategy is called OvA (One versus All) or OvR (One versus Rest).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Precompute the validation split using the multiclass labels</span>
        <span class="c1"># to ensure proper balancing of the classes.</span>
        <span class="n">validation_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_validation_split</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

        <span class="c1"># Use joblib to fit OvA in parallel.</span>
        <span class="c1"># Pick the random seed for each job outside of fit_binary to avoid</span>
        <span class="c1"># sharing the estimator random state between threads which could lead</span>
        <span class="c1"># to non-deterministic behavior</span>
        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="n">seeds</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">))</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                          <span class="o">**</span><span class="n">_joblib_parallel_args</span><span class="p">(</span><span class="n">require</span><span class="o">=</span><span class="s2">&quot;sharedmem&quot;</span><span class="p">))(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">fit_binary</span><span class="p">)(</span><span class="bp">self</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                                <span class="n">max_iter</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_expanded_class_weight</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                                <span class="mf">1.</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span>
                                <span class="n">validation_mask</span><span class="o">=</span><span class="n">validation_mask</span><span class="p">,</span>
                                <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">seed</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">seeds</span><span class="p">))</span>

        <span class="c1"># take the maximum of n_iter_ over every binary fit</span>
        <span class="n">n_iter_</span> <span class="o">=</span> <span class="mf">0.</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">n_iter_i</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">result</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">intercept</span>
            <span class="n">n_iter_</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">n_iter_</span><span class="p">,</span> <span class="n">n_iter_i</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">+=</span> <span class="n">n_iter_</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> <span class="n">n_iter_</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span>

    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Perform one epoch of stochastic gradient descent on given samples.</span>

<span class="sd">        Internally, this method uses ``max_iter = 1``. Therefore, it is not</span>
<span class="sd">        guaranteed that a minimum of the cost function is reached after calling</span>
<span class="sd">        it once. Matters such as objective convergence and early stopping</span>
<span class="sd">        should be handled by the user.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Subset of the training data</span>

<span class="sd">        y : numpy array, shape (n_samples,)</span>
<span class="sd">            Subset of the target values</span>

<span class="sd">        classes : array, shape (n_classes,)</span>
<span class="sd">            Classes across all calls to partial_fit.</span>
<span class="sd">            Can be obtained by via `np.unique(y_all)`, where y_all is the</span>
<span class="sd">            target vector of the entire dataset.</span>
<span class="sd">            This argument is required for the first call to partial_fit</span>
<span class="sd">            and can be omitted in the subsequent calls.</span>
<span class="sd">            Note that y doesn&#39;t need to contain all labels in `classes`.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,), optional</span>
<span class="sd">            Weights applied to individual samples.</span>
<span class="sd">            If not provided, uniform weights are assumed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">(</span><span class="n">for_partial_fit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;balanced&#39;</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;class_weight &#39;</span><span class="si">{0}</span><span class="s2">&#39; is not supported for &quot;</span>
                             <span class="s2">&quot;partial_fit. In order to use &#39;balanced&#39; weights,&quot;</span>
                             <span class="s2">&quot; use compute_class_weight(&#39;</span><span class="si">{0}</span><span class="s2">&#39;, classes, y). &quot;</span>
                             <span class="s2">&quot;In place of y you can us a large enough sample &quot;</span>
                             <span class="s2">&quot;of the full training set target to properly &quot;</span>
                             <span class="s2">&quot;estimate the class frequency distributions. &quot;</span>
                             <span class="s2">&quot;Pass the resulting weights as the class_weight &quot;</span>
                             <span class="s2">&quot;parameter.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                                 <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit linear model with Stochastic Gradient Descent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Training data</span>

<span class="sd">        y : numpy array, shape (n_samples,)</span>
<span class="sd">            Target values</span>

<span class="sd">        coef_init : array, shape (n_classes, n_features)</span>
<span class="sd">            The initial coefficients to warm-start the optimization.</span>

<span class="sd">        intercept_init : array, shape (n_classes,)</span>
<span class="sd">            The initial intercept to warm-start the optimization.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,), optional</span>
<span class="sd">            Weights applied to individual samples.</span>
<span class="sd">            If not provided, uniform weights are assumed. These weights will</span>
<span class="sd">            be multiplied with class_weight (passed through the</span>
<span class="sd">            constructor) if class_weight is specified</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                         <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                         <span class="n">coef_init</span><span class="o">=</span><span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="o">=</span><span class="n">intercept_init</span><span class="p">,</span>
                         <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SGDClassifier</span><span class="p">(</span><span class="n">BaseSGDClassifier</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear classifiers (SVM, logistic regression, a.o.) with SGD training.</span>

<span class="sd">    This estimator implements regularized linear models with stochastic</span>
<span class="sd">    gradient descent (SGD) learning: the gradient of the loss is estimated</span>
<span class="sd">    each sample at a time and the model is updated along the way with a</span>
<span class="sd">    decreasing strength schedule (aka learning rate). SGD allows minibatch</span>
<span class="sd">    (online/out-of-core) learning, see the partial_fit method.</span>
<span class="sd">    For best results using the default learning rate schedule, the data should</span>
<span class="sd">    have zero mean and unit variance.</span>

<span class="sd">    This implementation works with data represented as dense or sparse arrays</span>
<span class="sd">    of floating point values for the features. The model it fits can be</span>
<span class="sd">    controlled with the loss parameter; by default, it fits a linear support</span>
<span class="sd">    vector machine (SVM).</span>

<span class="sd">    The regularizer is a penalty added to the loss function that shrinks model</span>
<span class="sd">    parameters towards the zero vector using either the squared euclidean norm</span>
<span class="sd">    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the</span>
<span class="sd">    parameter update crosses the 0.0 value because of the regularizer, the</span>
<span class="sd">    update is truncated to 0.0 to allow for learning sparse models and achieve</span>
<span class="sd">    online feature selection.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;sgd&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : str, default: &#39;hinge&#39;</span>
<span class="sd">        The loss function to be used. Defaults to &#39;hinge&#39;, which gives a</span>
<span class="sd">        linear SVM.</span>

<span class="sd">        The possible options are &#39;hinge&#39;, &#39;log&#39;, &#39;modified_huber&#39;,</span>
<span class="sd">        &#39;squared_hinge&#39;, &#39;perceptron&#39;, or a regression loss: &#39;squared_loss&#39;,</span>
<span class="sd">        &#39;huber&#39;, &#39;epsilon_insensitive&#39;, or &#39;squared_epsilon_insensitive&#39;.</span>

<span class="sd">        The &#39;log&#39; loss gives logistic regression, a probabilistic classifier.</span>
<span class="sd">        &#39;modified_huber&#39; is another smooth loss that brings tolerance to</span>
<span class="sd">        outliers as well as probability estimates.</span>
<span class="sd">        &#39;squared_hinge&#39; is like hinge but is quadratically penalized.</span>
<span class="sd">        &#39;perceptron&#39; is the linear loss used by the perceptron algorithm.</span>
<span class="sd">        The other losses are designed for regression but can be useful in</span>
<span class="sd">        classification as well; see SGDRegressor for a description.</span>

<span class="sd">    penalty : str, &#39;none&#39;, &#39;l2&#39;, &#39;l1&#39;, or &#39;elasticnet&#39;</span>
<span class="sd">        The penalty (aka regularization term) to be used. Defaults to &#39;l2&#39;</span>
<span class="sd">        which is the standard regularizer for linear SVM models. &#39;l1&#39; and</span>
<span class="sd">        &#39;elasticnet&#39; might bring sparsity to the model (feature selection)</span>
<span class="sd">        not achievable with &#39;l2&#39;.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Constant that multiplies the regularization term. Defaults to 0.0001</span>
<span class="sd">        Also used to compute learning_rate when set to &#39;optimal&#39;.</span>

<span class="sd">    l1_ratio : float</span>
<span class="sd">        The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</span>
<span class="sd">        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.</span>
<span class="sd">        Defaults to 0.15.</span>

<span class="sd">    fit_intercept : bool</span>
<span class="sd">        Whether the intercept should be estimated or not. If False, the</span>
<span class="sd">        data is assumed to be already centered. Defaults to True.</span>

<span class="sd">    max_iter : int, optional (default=1000)</span>
<span class="sd">        The maximum number of passes over the training data (aka epochs).</span>
<span class="sd">        It only impacts the behavior in the ``fit`` method, and not the</span>
<span class="sd">        `partial_fit`.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    tol : float or None, optional (default=1e-3)</span>
<span class="sd">        The stopping criterion. If it is not None, the iterations will stop</span>
<span class="sd">        when (loss &gt; best_loss - tol) for ``n_iter_no_change`` consecutive</span>
<span class="sd">        epochs.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    shuffle : bool, optional</span>
<span class="sd">        Whether or not the training data should be shuffled after each epoch.</span>
<span class="sd">        Defaults to True.</span>

<span class="sd">    verbose : integer, optional</span>
<span class="sd">        The verbosity level</span>

<span class="sd">    epsilon : float</span>
<span class="sd">        Epsilon in the epsilon-insensitive loss functions; only if `loss` is</span>
<span class="sd">        &#39;huber&#39;, &#39;epsilon_insensitive&#39;, or &#39;squared_epsilon_insensitive&#39;.</span>
<span class="sd">        For &#39;huber&#39;, determines the threshold at which it becomes less</span>
<span class="sd">        important to get the prediction exactly right.</span>
<span class="sd">        For epsilon-insensitive, any differences between the current prediction</span>
<span class="sd">        and the correct label are ignored if they are less than this threshold.</span>

<span class="sd">    n_jobs : int or None, optional (default=None)</span>
<span class="sd">        The number of CPUs to use to do the OVA (One Versus All, for</span>
<span class="sd">        multi-class problems) computation.</span>
<span class="sd">        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.</span>
<span class="sd">        ``-1`` means using all processors. See :term:`Glossary &lt;n_jobs&gt;`</span>
<span class="sd">        for more details.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    learning_rate : string, optional</span>
<span class="sd">        The learning rate schedule:</span>

<span class="sd">        &#39;constant&#39;:</span>
<span class="sd">            eta = eta0</span>
<span class="sd">        &#39;optimal&#39;: [default]</span>
<span class="sd">            eta = 1.0 / (alpha * (t + t0))</span>
<span class="sd">            where t0 is chosen by a heuristic proposed by Leon Bottou.</span>
<span class="sd">        &#39;invscaling&#39;:</span>
<span class="sd">            eta = eta0 / pow(t, power_t)</span>
<span class="sd">        &#39;adaptive&#39;:</span>
<span class="sd">            eta = eta0, as long as the training keeps decreasing.</span>
<span class="sd">            Each time n_iter_no_change consecutive epochs fail to decrease the</span>
<span class="sd">            training loss by tol or fail to increase validation score by tol if</span>
<span class="sd">            early_stopping is True, the current learning rate is divided by 5.</span>

<span class="sd">    eta0 : double</span>
<span class="sd">        The initial learning rate for the &#39;constant&#39;, &#39;invscaling&#39; or</span>
<span class="sd">        &#39;adaptive&#39; schedules. The default value is 0.0 as eta0 is not used by</span>
<span class="sd">        the default schedule &#39;optimal&#39;.</span>

<span class="sd">    power_t : double</span>
<span class="sd">        The exponent for inverse scaling learning rate [default 0.5].</span>

<span class="sd">    early_stopping : bool, default=False</span>
<span class="sd">        Whether to use early stopping to terminate training when validation</span>
<span class="sd">        score is not improving. If set to True, it will automatically set aside</span>
<span class="sd">        a stratified fraction of training data as validation and terminate</span>
<span class="sd">        training when validation score is not improving by at least tol for</span>
<span class="sd">        n_iter_no_change consecutive epochs.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    validation_fraction : float, default=0.1</span>
<span class="sd">        The proportion of training data to set aside as validation set for</span>
<span class="sd">        early stopping. Must be between 0 and 1.</span>
<span class="sd">        Only used if early_stopping is True.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    n_iter_no_change : int, default=5</span>
<span class="sd">        Number of iterations with no improvement to wait before early stopping.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    class_weight : dict, {class_label: weight} or &quot;balanced&quot; or None, optional</span>
<span class="sd">        Preset for the class_weight fit parameter.</span>

<span class="sd">        Weights associated with classes. If not given, all classes</span>
<span class="sd">        are supposed to have weight one.</span>

<span class="sd">        The &quot;balanced&quot; mode uses the values of y to automatically adjust</span>
<span class="sd">        weights inversely proportional to class frequencies in the input data</span>
<span class="sd">        as ``n_samples / (n_classes * np.bincount(y))``</span>

<span class="sd">    warm_start : bool, optional</span>
<span class="sd">        When set to True, reuse the solution of the previous call to fit as</span>
<span class="sd">        initialization, otherwise, just erase the previous solution.</span>
<span class="sd">        See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">        Repeatedly calling fit or partial_fit when warm_start is True can</span>
<span class="sd">        result in a different solution than when calling fit a single time</span>
<span class="sd">        because of the way the data is shuffled.</span>
<span class="sd">        If a dynamic learning rate is used, the learning rate is adapted</span>
<span class="sd">        depending on the number of samples already seen. Calling ``fit`` resets</span>
<span class="sd">        this counter, while ``partial_fit`` will result in increasing the</span>
<span class="sd">        existing counter.</span>

<span class="sd">    average : bool or int, optional</span>
<span class="sd">        When set to True, computes the averaged SGD weights and stores the</span>
<span class="sd">        result in the ``coef_`` attribute. If set to an int greater than 1,</span>
<span class="sd">        averaging will begin once the total number of samples seen reaches</span>
<span class="sd">        average. So ``average=10`` will begin averaging after seeing 10</span>
<span class="sd">        samples.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array, shape (1, n_features) if n_classes == 2 else (n_classes,\</span>
<span class="sd">            n_features)</span>
<span class="sd">        Weights assigned to the features.</span>

<span class="sd">    intercept_ : array, shape (1,) if n_classes == 2 else (n_classes,)</span>
<span class="sd">        Constants in decision function.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        The actual number of iterations to reach the stopping criterion.</span>
<span class="sd">        For multiclass fits, it is the maximum over every binary fit.</span>

<span class="sd">    loss_function_ : concrete ``LossFunction``</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn import linear_model</span>
<span class="sd">    &gt;&gt;&gt; X = np.array([[-1, -1], [-2, -1], [1, 1], [2, 1]])</span>
<span class="sd">    &gt;&gt;&gt; Y = np.array([1, 1, 2, 2])</span>
<span class="sd">    &gt;&gt;&gt; clf = linear_model.SGDClassifier(max_iter=1000, tol=1e-3)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, Y)</span>
<span class="sd">    ... #doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    SGDClassifier(alpha=0.0001, average=False, class_weight=None,</span>
<span class="sd">           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,</span>
<span class="sd">           l1_ratio=0.15, learning_rate=&#39;optimal&#39;, loss=&#39;hinge&#39;, max_iter=1000,</span>
<span class="sd">           n_iter_no_change=5, n_jobs=None, penalty=&#39;l2&#39;, power_t=0.5,</span>
<span class="sd">           random_state=None, shuffle=True, tol=0.001, validation_fraction=0.1,</span>
<span class="sd">           verbose=0, warm_start=False)</span>

<span class="sd">    &gt;&gt;&gt; print(clf.predict([[-0.8, -1]]))</span>
<span class="sd">    [1]</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    sklearn.svm.LinearSVC, LogisticRegression, Perceptron</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;hinge&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span>
                 <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">DEFAULT_EPSILON</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;optimal&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">power_t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">,</span>
            <span class="n">power_t</span><span class="o">=</span><span class="n">power_t</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="n">validation_fraction</span><span class="o">=</span><span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">n_iter_no_change</span><span class="o">=</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="n">class_weight</span><span class="p">,</span>
            <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_check_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="s2">&quot;modified_huber&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">AttributeError</span><span class="p">(</span><span class="s2">&quot;probability estimates are not available for&quot;</span>
                                 <span class="s2">&quot; loss=</span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Probability estimates.</span>

<span class="sd">        This method is only available for log loss and modified Huber loss.</span>

<span class="sd">        Multiclass probability estimates are derived from binary (one-vs.-rest)</span>
<span class="sd">        estimates by simple normalization, as recommended by Zadrozny and</span>
<span class="sd">        Elkan.</span>

<span class="sd">        Binary probability estimates for loss=&quot;modified_huber&quot; are given by</span>
<span class="sd">        (clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions</span>
<span class="sd">        it is necessary to perform proper probability calibration by wrapping</span>
<span class="sd">        the classifier with</span>
<span class="sd">        :class:`sklearn.calibration.CalibratedClassifierCV` instead.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        array, shape (n_samples, n_classes)</span>
<span class="sd">            Returns the probability of the sample for each class in the model,</span>
<span class="sd">            where classes are ordered as they are in `self.classes_`.</span>

<span class="sd">        References</span>
<span class="sd">        ----------</span>
<span class="sd">        Zadrozny and Elkan, &quot;Transforming classifier scores into multiclass</span>
<span class="sd">        probability estimates&quot;, SIGKDD&#39;02,</span>
<span class="sd">        http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf</span>

<span class="sd">        The justification for the formula in the loss=&quot;modified_huber&quot;</span>
<span class="sd">        case is in the appendix B in:</span>
<span class="sd">        http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_proba</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_proba</span>

    <span class="k">def</span> <span class="nf">_predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;t_&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s2">&quot;log&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_proba_lr</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s2">&quot;modified_huber&quot;</span><span class="p">:</span>
            <span class="n">binary</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">)</span>
            <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
                <span class="n">prob2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">2</span><span class="p">))</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">prob2</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">scores</span>

            <span class="n">np</span><span class="o">.</span><span class="n">clip</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">prob</span><span class="p">)</span>
            <span class="n">prob</span> <span class="o">+=</span> <span class="mf">1.</span>
            <span class="n">prob</span> <span class="o">/=</span> <span class="mf">2.</span>

            <span class="k">if</span> <span class="n">binary</span><span class="p">:</span>
                <span class="n">prob2</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">prob</span>
                <span class="n">prob</span> <span class="o">=</span> <span class="n">prob2</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="c1"># the above might assign zero to all classes, which doesn&#39;t</span>
                <span class="c1"># normalize neatly; work around this to produce uniform</span>
                <span class="c1"># probabilities</span>
                <span class="n">prob_sum</span> <span class="o">=</span> <span class="n">prob</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="n">all_zero</span> <span class="o">=</span> <span class="p">(</span><span class="n">prob_sum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">all_zero</span><span class="p">):</span>
                    <span class="n">prob</span><span class="p">[</span><span class="n">all_zero</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="mi">1</span>
                    <span class="n">prob_sum</span><span class="p">[</span><span class="n">all_zero</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span>

                <span class="c1"># normalize</span>
                <span class="n">prob</span> <span class="o">/=</span> <span class="n">prob_sum</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">prob</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>

            <span class="k">return</span> <span class="n">prob</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s2">&quot;predict_(log_)proba only supported when&quot;</span>
                                      <span class="s2">&quot; loss=&#39;log&#39; or loss=&#39;modified_huber&#39; &quot;</span>
                                      <span class="s2">&quot;(</span><span class="si">%r</span><span class="s2"> given)&quot;</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Log of probability estimates.</span>

<span class="sd">        This method is only available for log loss and modified Huber loss.</span>

<span class="sd">        When loss=&quot;modified_huber&quot;, probability estimates may be hard zeros</span>
<span class="sd">        and ones, so taking the logarithm is not possible.</span>

<span class="sd">        See ``predict_proba`` for details.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : array-like, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        T : array-like, shape (n_samples, n_classes)</span>
<span class="sd">            Returns the log-probability of the sample for each class in the</span>
<span class="sd">            model, where classes are ordered as they are in</span>
<span class="sd">            `self.classes_`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_check_proba</span><span class="p">()</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_predict_log_proba</span>

    <span class="k">def</span> <span class="nf">_predict_log_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>


<span class="k">class</span> <span class="nc">BaseSGDRegressor</span><span class="p">(</span><span class="n">BaseSGD</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>

    <span class="n">loss_functions</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;squared_loss&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredLoss</span><span class="p">,</span> <span class="p">),</span>
        <span class="s2">&quot;huber&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">Huber</span><span class="p">,</span> <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
        <span class="s2">&quot;epsilon_insensitive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">EpsilonInsensitive</span><span class="p">,</span> <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
        <span class="s2">&quot;squared_epsilon_insensitive&quot;</span><span class="p">:</span> <span class="p">(</span><span class="n">SquaredEpsilonInsensitive</span><span class="p">,</span>
                                        <span class="n">DEFAULT_EPSILON</span><span class="p">),</span>
    <span class="p">}</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;squared_loss&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">DEFAULT_EPSILON</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;invscaling&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                 <span class="n">power_t</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">,</span>
            <span class="n">power_t</span><span class="o">=</span><span class="n">power_t</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="n">validation_fraction</span><span class="o">=</span><span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">n_iter_no_change</span><span class="o">=</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                     <span class="n">max_iter</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">coef_init</span><span class="p">,</span> <span class="n">intercept_init</span><span class="p">):</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">check_X_y</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;csr&quot;</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s1">&#39;C&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                         <span class="n">accept_large_sparse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># Allocate datastructures from input arguments</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_sample_weight</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_allocate_parameter_mem</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">coef_init</span><span class="p">,</span>
                                         <span class="n">intercept_init</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">n_features</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of features </span><span class="si">%d</span><span class="s2"> does not match previous &quot;</span>
                             <span class="s2">&quot;data </span><span class="si">%d</span><span class="s2">.&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;average_coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span>
                                          <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span>
                                          <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">order</span><span class="o">=</span><span class="s2">&quot;C&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_fit_regressor</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                            <span class="n">sample_weight</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">partial_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Perform one epoch of stochastic gradient descent on given samples.</span>

<span class="sd">        Internally, this method uses ``max_iter = 1``. Therefore, it is not</span>
<span class="sd">        guaranteed that a minimum of the cost function is reached after calling</span>
<span class="sd">        it once. Matters such as objective convergence and early stopping</span>
<span class="sd">        should be handled by the user.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Subset of training data</span>

<span class="sd">        y : numpy array of shape (n_samples,)</span>
<span class="sd">            Subset of target values</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,), optional</span>
<span class="sd">            Weights applied to individual samples.</span>
<span class="sd">            If not provided, uniform weights are assumed.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">(</span><span class="n">for_partial_fit</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                                 <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                                 <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                 <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
             <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_validate_params</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">coef_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">coef_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="k">if</span> <span class="n">intercept_init</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">intercept_init</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Clear iteration count for multiple call to fit.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_partial_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">coef_init</span><span class="p">,</span>
                          <span class="n">intercept_init</span><span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="o">&gt;</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>
                <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_iter</span><span class="p">):</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Maximum number of iteration reached before &quot;</span>
                          <span class="s2">&quot;convergence. Consider increasing max_iter to &quot;</span>
                          <span class="s2">&quot;improve the fit.&quot;</span><span class="p">,</span>
                          <span class="n">ConvergenceWarning</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">coef_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">intercept_init</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit linear model with Stochastic Gradient Descent.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>
<span class="sd">            Training data</span>

<span class="sd">        y : numpy array, shape (n_samples,)</span>
<span class="sd">            Target values</span>

<span class="sd">        coef_init : array, shape (n_features,)</span>
<span class="sd">            The initial coefficients to warm-start the optimization.</span>

<span class="sd">        intercept_init : array, shape (1,)</span>
<span class="sd">            The initial intercept to warm-start the optimization.</span>

<span class="sd">        sample_weight : array-like, shape (n_samples,), optional</span>
<span class="sd">            Weights applied to individual samples (1. for unweighted).</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        self : returns an instance of self.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
                         <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
                         <span class="n">coef_init</span><span class="o">=</span><span class="n">coef_init</span><span class="p">,</span>
                         <span class="n">intercept_init</span><span class="o">=</span><span class="n">intercept_init</span><span class="p">,</span>
                         <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_decision_function</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict using the linear model</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        array, shape (n_samples,)</span>
<span class="sd">           Predicted target values per element in X.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">check_is_fitted</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;t_&quot;</span><span class="p">,</span> <span class="s2">&quot;coef_&quot;</span><span class="p">,</span> <span class="s2">&quot;intercept_&quot;</span><span class="p">],</span> <span class="n">all_or_any</span><span class="o">=</span><span class="nb">all</span><span class="p">)</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csr&#39;</span><span class="p">)</span>

        <span class="n">scores</span> <span class="o">=</span> <span class="n">safe_sparse_dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">T</span><span class="p">,</span>
                                 <span class="n">dense_output</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span>
        <span class="k">return</span> <span class="n">scores</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict using the linear model</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {array-like, sparse matrix}, shape (n_samples, n_features)</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        array, shape (n_samples,)</span>
<span class="sd">           Predicted target values per element in X.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_decision_function</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_fit_regressor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span>
                       <span class="n">sample_weight</span><span class="p">,</span> <span class="n">max_iter</span><span class="p">):</span>
        <span class="n">dataset</span><span class="p">,</span> <span class="n">intercept_decay</span> <span class="o">=</span> <span class="n">make_dataset</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="n">loss_function</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_loss_function</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">penalty_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_penalty_type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">penalty</span><span class="p">)</span>
        <span class="n">learning_rate_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_learning_rate_type</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;t_&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">=</span> <span class="mf">1.0</span>

        <span class="n">validation_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_validation_split</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">validation_score_cb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_validation_score_cb</span><span class="p">(</span>
            <span class="n">validation_mask</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">)</span>

        <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
        <span class="c1"># numpy mtrand expects a C long which is a signed 32 bit integer under</span>
        <span class="c1"># Windows</span>
        <span class="n">seed</span> <span class="o">=</span> <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">iinfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span><span class="o">.</span><span class="n">max</span><span class="p">)</span>

        <span class="n">tol</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">tol</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">,</span> \
                <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span>\
                <span class="n">average_sgd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                            <span class="n">loss_function</span><span class="p">,</span>
                            <span class="n">penalty_type</span><span class="p">,</span>
                            <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
                            <span class="n">dataset</span><span class="p">,</span>
                            <span class="n">validation_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
                            <span class="n">validation_score_cb</span><span class="p">,</span>
                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">),</span>
                            <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span>
                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">),</span>
                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span>
                            <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">),</span>
                            <span class="n">seed</span><span class="p">,</span>
                            <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                            <span class="n">learning_rate_type</span><span class="p">,</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">eta0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">power_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span>
                            <span class="n">intercept_decay</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">average</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">-</span> <span class="mf">1.0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_coef_</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">average_intercept_</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_coef_</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">standard_intercept_</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">=</span> \
                <span class="n">plain_sgd</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
                          <span class="n">loss_function</span><span class="p">,</span>
                          <span class="n">penalty_type</span><span class="p">,</span>
                          <span class="n">alpha</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">l1_ratio</span><span class="p">,</span>
                          <span class="n">dataset</span><span class="p">,</span>
                          <span class="n">validation_mask</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">early_stopping</span><span class="p">,</span>
                          <span class="n">validation_score_cb</span><span class="p">,</span>
                          <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter_no_change</span><span class="p">),</span>
                          <span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="p">,</span>
                          <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fit_intercept</span><span class="p">),</span>
                          <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">),</span>
                          <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">),</span>
                          <span class="n">seed</span><span class="p">,</span>
                          <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span>
                          <span class="n">learning_rate_type</span><span class="p">,</span>
                          <span class="bp">self</span><span class="o">.</span><span class="n">eta0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">power_t</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">t_</span><span class="p">,</span>
                          <span class="n">intercept_decay</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">t_</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter_</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">SGDRegressor</span><span class="p">(</span><span class="n">BaseSGDRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Linear model fitted by minimizing a regularized empirical loss with SGD</span>

<span class="sd">    SGD stands for Stochastic Gradient Descent: the gradient of the loss is</span>
<span class="sd">    estimated each sample at a time and the model is updated along the way with</span>
<span class="sd">    a decreasing strength schedule (aka learning rate).</span>

<span class="sd">    The regularizer is a penalty added to the loss function that shrinks model</span>
<span class="sd">    parameters towards the zero vector using either the squared euclidean norm</span>
<span class="sd">    L2 or the absolute norm L1 or a combination of both (Elastic Net). If the</span>
<span class="sd">    parameter update crosses the 0.0 value because of the regularizer, the</span>
<span class="sd">    update is truncated to 0.0 to allow for learning sparse models and achieve</span>
<span class="sd">    online feature selection.</span>

<span class="sd">    This implementation works with data represented as dense numpy arrays of</span>
<span class="sd">    floating point values for the features.</span>

<span class="sd">    Read more in the :ref:`User Guide &lt;sgd&gt;`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    loss : str, default: &#39;squared_loss&#39;</span>
<span class="sd">        The loss function to be used. The possible values are &#39;squared_loss&#39;,</span>
<span class="sd">        &#39;huber&#39;, &#39;epsilon_insensitive&#39;, or &#39;squared_epsilon_insensitive&#39;</span>

<span class="sd">        The &#39;squared_loss&#39; refers to the ordinary least squares fit.</span>
<span class="sd">        &#39;huber&#39; modifies &#39;squared_loss&#39; to focus less on getting outliers</span>
<span class="sd">        correct by switching from squared to linear loss past a distance of</span>
<span class="sd">        epsilon. &#39;epsilon_insensitive&#39; ignores errors less than epsilon and is</span>
<span class="sd">        linear past that; this is the loss function used in SVR.</span>
<span class="sd">        &#39;squared_epsilon_insensitive&#39; is the same but becomes squared loss past</span>
<span class="sd">        a tolerance of epsilon.</span>

<span class="sd">    penalty : str, &#39;none&#39;, &#39;l2&#39;, &#39;l1&#39;, or &#39;elasticnet&#39;</span>
<span class="sd">        The penalty (aka regularization term) to be used. Defaults to &#39;l2&#39;</span>
<span class="sd">        which is the standard regularizer for linear SVM models. &#39;l1&#39; and</span>
<span class="sd">        &#39;elasticnet&#39; might bring sparsity to the model (feature selection)</span>
<span class="sd">        not achievable with &#39;l2&#39;.</span>

<span class="sd">    alpha : float</span>
<span class="sd">        Constant that multiplies the regularization term. Defaults to 0.0001</span>
<span class="sd">        Also used to compute learning_rate when set to &#39;optimal&#39;.</span>

<span class="sd">    l1_ratio : float</span>
<span class="sd">        The Elastic Net mixing parameter, with 0 &lt;= l1_ratio &lt;= 1.</span>
<span class="sd">        l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.</span>
<span class="sd">        Defaults to 0.15.</span>

<span class="sd">    fit_intercept : bool</span>
<span class="sd">        Whether the intercept should be estimated or not. If False, the</span>
<span class="sd">        data is assumed to be already centered. Defaults to True.</span>

<span class="sd">    max_iter : int, optional (default=1000)</span>
<span class="sd">        The maximum number of passes over the training data (aka epochs).</span>
<span class="sd">        It only impacts the behavior in the ``fit`` method, and not the</span>
<span class="sd">        `partial_fit`.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    tol : float or None, optional (default=1e-3)</span>
<span class="sd">        The stopping criterion. If it is not None, the iterations will stop</span>
<span class="sd">        when (loss &gt; best_loss - tol) for ``n_iter_no_change`` consecutive</span>
<span class="sd">        epochs.</span>

<span class="sd">        .. versionadded:: 0.19</span>

<span class="sd">    shuffle : bool, optional</span>
<span class="sd">        Whether or not the training data should be shuffled after each epoch.</span>
<span class="sd">        Defaults to True.</span>

<span class="sd">    verbose : integer, optional</span>
<span class="sd">        The verbosity level.</span>

<span class="sd">    epsilon : float</span>
<span class="sd">        Epsilon in the epsilon-insensitive loss functions; only if `loss` is</span>
<span class="sd">        &#39;huber&#39;, &#39;epsilon_insensitive&#39;, or &#39;squared_epsilon_insensitive&#39;.</span>
<span class="sd">        For &#39;huber&#39;, determines the threshold at which it becomes less</span>
<span class="sd">        important to get the prediction exactly right.</span>
<span class="sd">        For epsilon-insensitive, any differences between the current prediction</span>
<span class="sd">        and the correct label are ignored if they are less than this threshold.</span>

<span class="sd">    random_state : int, RandomState instance or None, optional (default=None)</span>
<span class="sd">        The seed of the pseudo random number generator to use when shuffling</span>
<span class="sd">        the data.  If int, random_state is the seed used by the random number</span>
<span class="sd">        generator; If RandomState instance, random_state is the random number</span>
<span class="sd">        generator; If None, the random number generator is the RandomState</span>
<span class="sd">        instance used by `np.random`.</span>

<span class="sd">    learning_rate : string, optional</span>
<span class="sd">        The learning rate schedule:</span>

<span class="sd">        &#39;constant&#39;:</span>
<span class="sd">            eta = eta0</span>
<span class="sd">        &#39;optimal&#39;:</span>
<span class="sd">            eta = 1.0 / (alpha * (t + t0))</span>
<span class="sd">            where t0 is chosen by a heuristic proposed by Leon Bottou.</span>
<span class="sd">        &#39;invscaling&#39;: [default]</span>
<span class="sd">            eta = eta0 / pow(t, power_t)</span>
<span class="sd">        &#39;adaptive&#39;:</span>
<span class="sd">            eta = eta0, as long as the training keeps decreasing.</span>
<span class="sd">            Each time n_iter_no_change consecutive epochs fail to decrease the</span>
<span class="sd">            training loss by tol or fail to increase validation score by tol if</span>
<span class="sd">            early_stopping is True, the current learning rate is divided by 5.</span>

<span class="sd">    eta0 : double</span>
<span class="sd">        The initial learning rate for the &#39;constant&#39;, &#39;invscaling&#39; or</span>
<span class="sd">        &#39;adaptive&#39; schedules. The default value is 0.01.</span>

<span class="sd">    power_t : double</span>
<span class="sd">        The exponent for inverse scaling learning rate [default 0.5].</span>

<span class="sd">    early_stopping : bool, default=False</span>
<span class="sd">        Whether to use early stopping to terminate training when validation</span>
<span class="sd">        score is not improving. If set to True, it will automatically set aside</span>
<span class="sd">        a fraction of training data as validation and terminate</span>
<span class="sd">        training when validation score is not improving by at least tol for</span>
<span class="sd">        n_iter_no_change consecutive epochs.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    validation_fraction : float, default=0.1</span>
<span class="sd">        The proportion of training data to set aside as validation set for</span>
<span class="sd">        early stopping. Must be between 0 and 1.</span>
<span class="sd">        Only used if early_stopping is True.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    n_iter_no_change : int, default=5</span>
<span class="sd">        Number of iterations with no improvement to wait before early stopping.</span>

<span class="sd">        .. versionadded:: 0.20</span>

<span class="sd">    warm_start : bool, optional</span>
<span class="sd">        When set to True, reuse the solution of the previous call to fit as</span>
<span class="sd">        initialization, otherwise, just erase the previous solution.</span>
<span class="sd">        See :term:`the Glossary &lt;warm_start&gt;`.</span>

<span class="sd">        Repeatedly calling fit or partial_fit when warm_start is True can</span>
<span class="sd">        result in a different solution than when calling fit a single time</span>
<span class="sd">        because of the way the data is shuffled.</span>
<span class="sd">        If a dynamic learning rate is used, the learning rate is adapted</span>
<span class="sd">        depending on the number of samples already seen. Calling ``fit`` resets</span>
<span class="sd">        this counter, while ``partial_fit``  will result in increasing the</span>
<span class="sd">        existing counter.</span>

<span class="sd">    average : bool or int, optional</span>
<span class="sd">        When set to True, computes the averaged SGD weights and stores the</span>
<span class="sd">        result in the ``coef_`` attribute. If set to an int greater than 1,</span>
<span class="sd">        averaging will begin once the total number of samples seen reaches</span>
<span class="sd">        average. So ``average=10`` will begin averaging after seeing 10</span>
<span class="sd">        samples.</span>

<span class="sd">    Attributes</span>
<span class="sd">    ----------</span>
<span class="sd">    coef_ : array, shape (n_features,)</span>
<span class="sd">        Weights assigned to the features.</span>

<span class="sd">    intercept_ : array, shape (1,)</span>
<span class="sd">        The intercept term.</span>

<span class="sd">    average_coef_ : array, shape (n_features,)</span>
<span class="sd">        Averaged weights assigned to the features.</span>

<span class="sd">    average_intercept_ : array, shape (1,)</span>
<span class="sd">        The averaged intercept term.</span>

<span class="sd">    n_iter_ : int</span>
<span class="sd">        The actual number of iterations to reach the stopping criterion.</span>

<span class="sd">    Examples</span>
<span class="sd">    --------</span>
<span class="sd">    &gt;&gt;&gt; import numpy as np</span>
<span class="sd">    &gt;&gt;&gt; from sklearn import linear_model</span>
<span class="sd">    &gt;&gt;&gt; n_samples, n_features = 10, 5</span>
<span class="sd">    &gt;&gt;&gt; rng = np.random.RandomState(0)</span>
<span class="sd">    &gt;&gt;&gt; y = rng.randn(n_samples)</span>
<span class="sd">    &gt;&gt;&gt; X = rng.randn(n_samples, n_features)</span>
<span class="sd">    &gt;&gt;&gt; clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)</span>
<span class="sd">    &gt;&gt;&gt; clf.fit(X, y)</span>
<span class="sd">    ... #doctest: +NORMALIZE_WHITESPACE</span>
<span class="sd">    SGDRegressor(alpha=0.0001, average=False, early_stopping=False,</span>
<span class="sd">           epsilon=0.1, eta0=0.01, fit_intercept=True, l1_ratio=0.15,</span>
<span class="sd">           learning_rate=&#39;invscaling&#39;, loss=&#39;squared_loss&#39;, max_iter=1000,</span>
<span class="sd">           n_iter_no_change=5, penalty=&#39;l2&#39;, power_t=0.25, random_state=None,</span>
<span class="sd">           shuffle=True, tol=0.001, validation_fraction=0.1, verbose=0,</span>
<span class="sd">           warm_start=False)</span>

<span class="sd">    See also</span>
<span class="sd">    --------</span>
<span class="sd">    Ridge, ElasticNet, Lasso, sklearn.svm.SVR</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;squared_loss&quot;</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">,</span>
                 <span class="n">l1_ratio</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
                 <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">DEFAULT_EPSILON</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="s2">&quot;invscaling&quot;</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
                 <span class="n">power_t</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">validation_fraction</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
                 <span class="n">n_iter_no_change</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">average</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="n">penalty</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">l1_ratio</span><span class="o">=</span><span class="n">l1_ratio</span><span class="p">,</span>
            <span class="n">fit_intercept</span><span class="o">=</span><span class="n">fit_intercept</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="n">tol</span><span class="p">,</span>
            <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="n">epsilon</span><span class="p">,</span>
            <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span> <span class="n">eta0</span><span class="o">=</span><span class="n">eta0</span><span class="p">,</span>
            <span class="n">power_t</span><span class="o">=</span><span class="n">power_t</span><span class="p">,</span> <span class="n">early_stopping</span><span class="o">=</span><span class="n">early_stopping</span><span class="p">,</span>
            <span class="n">validation_fraction</span><span class="o">=</span><span class="n">validation_fraction</span><span class="p">,</span>
            <span class="n">n_iter_no_change</span><span class="o">=</span><span class="n">n_iter_no_change</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">,</span>
            <span class="n">average</span><span class="o">=</span><span class="n">average</span><span class="p">)</span>
</pre></div>

           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, MIT Data To AI Lab

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>